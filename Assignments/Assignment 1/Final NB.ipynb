{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-01T05:44:54.049954Z",
     "iopub.status.busy": "2025-03-01T05:44:54.049766Z",
     "iopub.status.idle": "2025-03-01T05:45:01.627893Z",
     "shell.execute_reply": "2025-03-01T05:45:01.627176Z",
     "shell.execute_reply.started": "2025-03-01T05:44:54.049936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Importing Helper Modules**\n",
    "\n",
    "import torch  # üß† Core PyTorch library for tensor operations and neural networks\n",
    "import torch.nn as nn  # üèóÔ∏è Neural network components (layers, loss functions)\n",
    "import torch.optim as optim  # ‚öôÔ∏è Optimization algorithms (SGD, Adam, etc.)\n",
    "import torchvision  # üé® Computer vision utilities and datasets\n",
    "import torchvision.transforms as transforms  # üñºÔ∏è Data transformations (normalization, augmentation)\n",
    "from torch.utils.data import DataLoader  # üöö For loading and batching data\n",
    "import matplotlib.pyplot as plt  # üìä Visualization for losses and accuracies\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:05.277009Z",
     "iopub.status.busy": "2025-03-01T05:45:05.276678Z",
     "iopub.status.idle": "2025-03-01T05:45:05.280894Z",
     "shell.execute_reply": "2025-03-01T05:45:05.279898Z",
     "shell.execute_reply.started": "2025-03-01T05:45:05.276981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:07.132525Z",
     "iopub.status.busy": "2025-03-01T05:45:07.132233Z",
     "iopub.status.idle": "2025-03-01T05:45:07.139285Z",
     "shell.execute_reply": "2025-03-01T05:45:07.138270Z",
     "shell.execute_reply.started": "2025-03-01T05:45:07.132503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 1: Data Loading and Preprocessing**\n",
    "\n",
    "# üõ†Ô∏è TODO: Complete the data loading code üß©\n",
    "def load_mnist_data(batch_size=64):\n",
    "    \"\"\"\n",
    "    üì¶ **Load and preprocess the MNIST dataset.**\n",
    "    üìú **Returns:** train_loader and test_loader üéØ\n",
    "    \"\"\"\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.ToTensor(),  # üîÑ Convert images to tensors üìä\n",
    "    #     transforms.Normalize((0.1307,), (0.3081,))  # ‚öñÔ∏è Normalize with mean and std for MNIST dataset\n",
    "    # ])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # resize to OG VGG16 input with 1 channel\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)), # random rotation of +- 15 degrees, with 10% shift at random\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # resize to OG VGG16 input with 3 channels\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])\n",
    "\n",
    "    # üõ†Ô∏è **TODO: Load MNIST training and test datasets** üñºÔ∏è\n",
    "    # üìå Hint: Use `torchvision.datasets.MNIST` for dataset loading üì•\n",
    "    #          Use `torch.utils.data.DataLoader` for creating data loaders üîÑ\n",
    "\n",
    "    # train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    # test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=test_transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Example (not yet implemented):\n",
    "    # train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    # test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader  # üöö Return the loaders üì¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:11.464290Z",
     "iopub.status.busy": "2025-03-01T05:45:11.463996Z",
     "iopub.status.idle": "2025-03-01T05:45:11.469184Z",
     "shell.execute_reply": "2025-03-01T05:45:11.468281Z",
     "shell.execute_reply.started": "2025-03-01T05:45:11.464267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 2: Custom Dropout Implementation**\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    üõ†Ô∏è TODO: Implement custom dropout layer üéØ\n",
    "\n",
    "    üìú **Requirements:**\n",
    "    1Ô∏è‚É£ Initialize with **dropout probability** `p` üé≤\n",
    "    2Ô∏è‚É£ Implement **forward pass** with proper scaling üîÑ\n",
    "    3Ô∏è‚É£ **Only drop** units during **training** (`self.training` flag) üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        super(CustomDropout, self).__init__()\n",
    "        # üé≤ Store dropout probability (p between 0 and 1)\n",
    "        self.p = p\n",
    "        # pass  # üöß Initialization complete! Time to implement the logic üõ†Ô∏è\n",
    "\n",
    "    def forward(self, x):\n",
    "        # üîÑ **TODO: Implement forward pass**\n",
    "        if self.training:  # üèãÔ∏è‚Äç‚ôÇÔ∏è Drop units only during training mode\n",
    "            # pass  # üöß Work in progress! Apply dropout logic üß™\n",
    "            mask = torch.bernoulli(torch.ones_like(x) * (1 - self.p)) # using a bernoulli distribution for generating random 0 or 1, p probability of mask being 0\n",
    "            # alternate implementation\n",
    "            # mask = (torch.rand_like(x) > self.p).float()\n",
    "            return x * mask / (1 - self.p) # dividing by (1-p) is crucial so that the expected value of the activations ultimately remain unchanged\n",
    "        return x  # üîÑ Return the (possibly dropped) output ‚ú®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:14.739490Z",
     "iopub.status.busy": "2025-03-01T05:45:14.739191Z",
     "iopub.status.idle": "2025-03-01T05:45:14.747119Z",
     "shell.execute_reply": "2025-03-01T05:45:14.746138Z",
     "shell.execute_reply.started": "2025-03-01T05:45:14.739467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 3: Custom BatchNorm2d Implementation**\n",
    "\n",
    "class CustomBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    üõ†Ô∏è TODO: Implement custom 2D batch normalization üîÑ\n",
    "\n",
    "    üìú **Requirements:**\n",
    "    1Ô∏è‚É£ Initialize **running mean**, **variance**, **gamma (scale)**, and **beta (shift)** ‚öñÔ∏è\n",
    "    2Ô∏è‚É£ Implement **forward pass** with proper normalization ‚ú®\n",
    "    3Ô∏è‚É£ Track **running statistics** during training üìä\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(CustomBatchNorm2d, self).__init__()\n",
    "        # üõ†Ô∏è **TODO: Initialize parameters and buffers**\n",
    "        # pass  # üöß Work in progress üöÄ\n",
    "        self.num_features = num_features # number of channels\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features)) # scaling factor\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features)) # shifting factor\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features)) # simple buffer to store the running values, optional\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # üîÑ **TODO: Implement forward pass for batch normalization**\n",
    "        # Steps:\n",
    "        # 1Ô∏è‚É£ Calculate batch mean and variance üìä\n",
    "        # 2Ô∏è‚É£ Normalize the input üéØ\n",
    "        # 3Ô∏è‚É£ Apply learnable parameters (gamma and beta) ‚öôÔ∏è\n",
    "        # 4Ô∏è‚É£ Update running statistics during training üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "        # pass  # üöß Normalize and return the output üß™\n",
    "        if self.training:\n",
    "            batch_mean = x.mean([0, 2, 3]) # mean and variance is per channel, the input here is 4D (batch_size, num_channels, height, width) and we are applying across (0, 2, 3) indexes in the given array\n",
    "            batch_var = x.var([0, 2, 3], unbiased=False)\n",
    "\n",
    "            self.running_mean.mul_(1 - self.momentum).add_(self.momentum * batch_mean)\n",
    "            self.running_var.mul_(1 - self.momentum).add_(self.momentum * batch_var)\n",
    "\n",
    "            mean = batch_mean.view(1, -1, 1, 1)\n",
    "            var = batch_var.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            mean = self.running_mean.view(1, -1, 1, 1)\n",
    "            var = self.running_var.view(1, -1, 1, 1)\n",
    "\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        gamma = self.gamma.view(1, -1, 1, 1)\n",
    "        beta = self.beta.view(1, -1, 1, 1)\n",
    "        ret_val = gamma * x_normalized + beta\n",
    "\n",
    "        return ret_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:17.574230Z",
     "iopub.status.busy": "2025-03-01T05:45:17.573941Z",
     "iopub.status.idle": "2025-03-01T05:45:17.578201Z",
     "shell.execute_reply": "2025-03-01T05:45:17.577437Z",
     "shell.execute_reply.started": "2025-03-01T05:45:17.574209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    üõ†Ô∏è TODO: Implement custom ReLU activation function ‚ú®\n",
    "\n",
    "    üìú **Requirements:**\n",
    "    1Ô∏è‚É£ Apply ReLU manually using tensor operations (avoid using `F.relu`) üîÑ\n",
    "    2Ô∏è‚É£ Output should replace all negative values with 0 (ReLU behavior) üßπ\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # üîÑ **TODO: Implement forward pass for ReLU**\n",
    "        # Hint: Use `torch.max` to replace all negative values with 0 üéØ\n",
    "        # pass  # üöß Replace and return the ReLU-activated output ‚ö°\n",
    "        return torch.max(torch.zeros_like(x), x) # normal ReLU = max(0, x) implementation but vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:19.399795Z",
     "iopub.status.busy": "2025-03-01T05:45:19.399465Z",
     "iopub.status.idle": "2025-03-01T05:45:19.405432Z",
     "shell.execute_reply": "2025-03-01T05:45:19.404478Z",
     "shell.execute_reply.started": "2025-03-01T05:45:19.399769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomMaxPooling2d(nn.Module):\n",
    "    \"\"\"\n",
    "    üõ†Ô∏è TODO: Implement custom 2D MaxPooling layer üèä\n",
    "\n",
    "    üìú **Requirements:**\n",
    "    1Ô∏è‚É£ Implement a max-pooling operation with a given kernel size and stride üìê\n",
    "    2Ô∏è‚É£ Return the maximum value in each pooling window üåä\n",
    "    3Ô∏è‚É£ Ensure it supports both training and evaluation modes üîÑ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        super(CustomMaxPooling2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        # üîÑ **TODO: Implement forward pass for max-pooling**\n",
    "        # Hint: Use `unfold` to break the input into windows and compute the max for each window üîç\n",
    "        # pass  # üöß Pool and return the reduced output üèä‚Äç‚ôÇÔ∏è\n",
    "        N, C, H, W = x.shape # batch, channels, height, width\n",
    "        out_H = (H - self.kernel_size) // self.stride + 1 # normal theory formula O = floor ((I + 2P - K) / S) + 1\n",
    "        out_W = (W - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        x_unfold = F.unfold(x, kernel_size = self.kernel_size, stride = self.stride)\n",
    "        x_unfold = x_unfold.view(N, C, self.kernel_size * self.kernel_size, -1)\n",
    "\n",
    "        out, _ = x_unfold.max(dim = 2)\n",
    "        out = out.view(N, C, out_H, out_W)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:21.461422Z",
     "iopub.status.busy": "2025-03-01T05:45:21.461073Z",
     "iopub.status.idle": "2025-03-01T05:45:21.470049Z",
     "shell.execute_reply": "2025-03-01T05:45:21.469099Z",
     "shell.execute_reply.started": "2025-03-01T05:45:21.461384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 4: Custom VGG16 Model Implementation**\n",
    "class CustomVGG16(nn.Module):\n",
    "    \"\"\"\n",
    "    üìú Custom VGG16-like Model with:\n",
    "    1Ô∏è‚É£ Convolutional blocks using nn.Conv2d, CustomBatchNorm2d, and CustomDropout üîÑ\n",
    "    2Ô∏è‚É£ ReLU activation ‚ö° and MaxPooling üèä\n",
    "    3Ô∏è‚É£ Fully connected layers at the end\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):  # num_classes = 10 for MNIST\n",
    "        super(CustomVGG16, self).__init__()\n",
    "        # üî® **TODO: Define your layers here**\n",
    "        # Example: self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        # pass  # üöß Work in progress üöÄ\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(64),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(64),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPooling2d(),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(128),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(128),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPooling2d(),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(256),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(256),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(256),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPooling2d(),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPooling2d(),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            CustomBatchNorm2d(512),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPooling2d()\n",
    "\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            CustomReLU(),\n",
    "            CustomDropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            CustomReLU(),\n",
    "            CustomDropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # üîÑ **TODO: Implement the forward pass** üí°\n",
    "        # Example: x = self.conv1(x)\n",
    "        # pass  # üöß Process the input and return the output üéØ\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:26.050449Z",
     "iopub.status.busy": "2025-03-01T05:45:26.050151Z",
     "iopub.status.idle": "2025-03-01T05:45:26.059469Z",
     "shell.execute_reply": "2025-03-01T05:45:26.058521Z",
     "shell.execute_reply.started": "2025-03-01T05:45:26.050426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 5: Training Functions**\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    üõ†Ô∏è TODO: Implement training loop for one epoch üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "    \"\"\"\n",
    "    model.train()  # üìà Switch to training mode\n",
    "    running_loss = 0.0  # üí∞ Track the cumulative loss\n",
    "    correct = 0  # ‚úÖ Correct predictions counter\n",
    "    total = 0  # üìä Total samples counter\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, (data, target) in pbar:  # üîÑ Loop through batches\n",
    "        # üìå Your code here (e.g., forward pass, loss calculation, backward pass, optimizer step)\n",
    "        # pass\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # clear from previous batch\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predicted = output.argmax(dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "\n",
    "    # üìä Return average loss and accuracy for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total if total > 0 else 0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    üß™ TODO: Implement evaluation loop üîç\n",
    "    \"\"\"\n",
    "    model.eval()  # üîï Switch to evaluation mode (no gradients)\n",
    "    test_loss = 0  # üí∞ Track cumulative test loss\n",
    "    correct = 0  # ‚úÖ Correct predictions counter\n",
    "    total = 0  # üìä Total samples counter\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():  # üö´ No gradient calculation for evaluation\n",
    "        # üìå Your code here (e.g., forward pass, loss calculation, accuracy calculation)\n",
    "        # pass\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predicted = output.argmax(dim=1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': 100. * correct / total if total > 0 else 0\n",
    "            })\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total if total > 0 else 0\n",
    "    # üìä Return average test loss and accuracy\n",
    "    return avg_loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T05:45:32.104748Z",
     "iopub.status.busy": "2025-03-01T05:45:32.104391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Part 6: Main Training Loop**\n",
    "\n",
    "def main():\n",
    "    # ‚öôÔ∏è **Hyperparameters**\n",
    "    BATCH_SIZE = 16  # üì¶ Batch size for data loading\n",
    "    EPOCHS = 2  # üîÑ Number of training epochs\n",
    "    LEARNING_RATE = 0.001  # üöÄ Learning rate for optimizer\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # ‚ö° Use GPU if available\n",
    "\n",
    "    # üìä **Load data**\n",
    "    train_loader, test_loader = load_mnist_data(BATCH_SIZE)\n",
    "\n",
    "    # üõ†Ô∏è **Initialize model, criterion, optimizer**\n",
    "    model = CustomVGG16().to(DEVICE)  # üñ•Ô∏è Move model to the selected device\n",
    "    criterion = nn.CrossEntropyLoss()  # üéØ Loss function for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # üöÄ Adam optimizer for better convergence\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "    # üîÑ **Training loop**\n",
    "    train_losses = []  # üìâ Track training losses\n",
    "    test_losses = []  # üìâ Track test losses\n",
    "    train_accs = []  # üìä Track training accuracy\n",
    "    test_accs = []  # üìä Track test accuracy\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # üèãÔ∏è‚Äç‚ôÇÔ∏è **TODO: Implement main training loop**\n",
    "        print(f\"üåü Epoch {epoch+1}/{EPOCHS}\")  # üïí Display current epoch\n",
    "        # Example Steps:\n",
    "        # 1Ô∏è‚É£ Train for one epoch\n",
    "        # 2Ô∏è‚É£ Evaluate on test set\n",
    "        # 3Ô∏è‚É£ Record losses and accuracies\n",
    "        # 4Ô∏è‚É£ Print progress üí¨\n",
    "        # pass\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "        print(f\"üîÑ Learning rate changed to: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} finished:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    # üìà **Plot results**\n",
    "    # üõ†Ô∏è **TODO: Create loss and accuracy plots**\n",
    "    # Example: plt.plot(train_losses), plt.plot(test_losses), etc.\n",
    "    # pass  # üé® Generate and display plots üìä\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-27T13:40:29.001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For comparison with PyTorch\n",
    "class PyTorchVGG16(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG16 using PyTorch's built-in layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(PyTorchVGG16, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_loader)} \"\n",
    "                  f\"Loss: {loss.item()} \"\n",
    "                  f\"Accuracy: {100. * correct / total}\")\n",
    "\n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    return test_loss / len(test_loader), 100. * correct / total\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load data\n",
    "    train_loader, test_loader = load_mnist_data(BATCH_SIZE)\n",
    "\n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = PyTorchVGG16().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "\n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, DEVICE\n",
    "        )\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
