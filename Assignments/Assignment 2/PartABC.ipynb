{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code contains necessary imports and installs so that we can run it on **Kaggle Notebook** specifically.  \n",
    "Please note that we need to make changes to `'directory'` and `'path'` variables wherever necessary.  \n",
    "No guarantee can be given if the code will run from another setup unlike Kaggle Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:19:16.140084Z",
     "iopub.status.busy": "2025-04-10T04:19:16.139776Z",
     "iopub.status.idle": "2025-04-10T04:20:35.157921Z",
     "shell.execute_reply": "2025-04-10T04:20:35.156948Z",
     "shell.execute_reply.started": "2025-04-10T04:19:16.140061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2386a8aa6e75ec3a643a3148d44ba89154bb79966d08ce6326b6f8e916fc0d88\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: fsspec, rouge_score, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0 rouge_score-0.1.2\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Run this is working on Kaggle (for local just pip install the requirements.txt)\n",
    "!pip install evaluate rouge_score transformers\n",
    "!pip install --upgrade nltk\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:20:45.828122Z",
     "iopub.status.busy": "2025-04-10T04:20:45.827804Z",
     "iopub.status.idle": "2025-04-10T04:21:12.636657Z",
     "shell.execute_reply": "2025-04-10T04:21:12.635848Z",
     "shell.execute_reply.started": "2025-04-10T04:20:45.828093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import evaluate\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import(\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    ViTModel,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    ViTFeatureExtractor,\n",
    "    BertModel,\n",
    "    BertTokenizer\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers.image_utils import load_image\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-10T04:24:43.720762Z",
     "iopub.status.busy": "2025-04-10T04:24:43.719941Z",
     "iopub.status.idle": "2025-04-10T04:24:44.526364Z",
     "shell.execute_reply": "2025-04-10T04:24:44.525625Z",
     "shell.execute_reply.started": "2025-04-10T04:24:43.720731Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
      "[nltk_data] Downloading package omw-1.4 to /kaggle/working/...\n",
      "[nltk_data] Downloading package punkt to /kaggle/working/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /kaggle/working/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# run this if working on Kaggle {by changing path as required} (it should run automatically on local {not tested though})\n",
    "# !mkdir -p /kaggle/working/nltk_data/corpora\n",
    "nltk_data_dir = \"/kaggle/working/\"\n",
    "nltk.download(\"wordnet\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"omw-1.4\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:25:24.985517Z",
     "iopub.status.busy": "2025-04-10T04:25:24.984806Z",
     "iopub.status.idle": "2025-04-10T04:25:26.351118Z",
     "shell.execute_reply": "2025-04-10T04:25:26.350253Z",
     "shell.execute_reply.started": "2025-04-10T04:25:24.985493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/working/corpora/wordnet.zip\n",
      "   creating: /kaggle/working/corpora/wordnet/\n",
      "  inflating: /kaggle/working/corpora/wordnet/lexnames  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.verb  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.adv  \n",
      "  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.verb  \n",
      "  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.adj  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.adj  \n",
      "  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/README  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.sense  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.noun  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.adv  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.noun  \n",
      "  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n",
      "Archive:  /kaggle/working/corpora/omw-1.4.zip\n",
      "   creating: /kaggle/working/corpora/omw-1.4/\n",
      "   creating: /kaggle/working/corpora/omw-1.4/fin/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fin/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fin/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fin/wn-data-fin.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/heb/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/heb/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/heb/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/heb/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/heb/wn-data-heb.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/slv/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slv/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slv/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slv/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slv/wn-data-slv.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/ita/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ita/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ita/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ita/wn-data-ita.tab  \n",
      " extracting: /kaggle/working/corpora/omw-1.4/ita/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/nor/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nor/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nor/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nor/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nor/wn-data-nno.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nor/wn-data-nob.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/als/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/als/wn-data-als.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/als/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/als/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/als/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/pol/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/pol/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/pol/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/pol/wn-data-pol.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/hrv/\n",
      " extracting: /kaggle/working/corpora/omw-1.4/hrv/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/hrv/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/hrv/wn-data-hrv.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/hrv/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/citation.bib  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/iwn/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/iwn/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/iwn/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/iwn/wn-data-ita.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/iwn/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/nld/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nld/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nld/wn-data-nld.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/nld/citation.bib  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/ron/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ron/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ron/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ron/wn-data-ron.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ron/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/arb/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/arb/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/arb/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/arb/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/arb/wn-data-arb.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/isl/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/isl/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/isl/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/isl/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/isl/wn-data-isl.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/swe/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/swe/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/swe/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/swe/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/swe/wn-data-swe.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/por/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/por/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/por/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/por/wn-data-por.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/por/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/cow/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/cow/wn-data-cmn.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/cow/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/cow/citation.bib  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/jpn/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/jpn/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/jpn/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/jpn/README  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/jpn/wn-data-jpn.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/dan/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/dan/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/dan/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/dan/wn-data-dan.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/slk/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slk/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slk/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slk/wn-data-slk.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slk/wn-data-lit.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/slk/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/bul/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/bul/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/bul/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/bul/wn-data-bul.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/bul/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/mcr/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/wn-data-eus.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/wn-data-cat.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/wn-data-glg.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/mcr/wn-data-spa.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/ell/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ell/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ell/wn-data-ell.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/ell/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/msa/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/msa/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/msa/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/msa/wn-data-zsm.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/msa/wn-data-ind.tab  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/msa/README  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/fra/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fra/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fra/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/fra/wn-data-fra.tab  \n",
      "   creating: /kaggle/working/corpora/omw-1.4/tha/\n",
      "  inflating: /kaggle/working/corpora/omw-1.4/tha/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/tha/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/omw-1.4/tha/wn-data-tha.tab  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\n",
    "!unzip -o /kaggle/working/corpora/omw-1.4.zip -d /kaggle/working/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:26:51.235688Z",
     "iopub.status.busy": "2025-04-10T04:26:51.235023Z",
     "iopub.status.idle": "2025-04-10T04:26:51.240070Z",
     "shell.execute_reply": "2025-04-10T04:26:51.239280Z",
     "shell.execute_reply.started": "2025-04-10T04:26:51.235657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/kaggle/working/corpora\")\n",
    "nltk.data.path.append(\"/kaggle/working/tokenizers\")\n",
    "nltk.data.path.append(\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:26:53.785472Z",
     "iopub.status.busy": "2025-04-10T04:26:53.784740Z",
     "iopub.status.idle": "2025-04-10T04:26:53.789721Z",
     "shell.execute_reply": "2025-04-10T04:26:53.788879Z",
     "shell.execute_reply.started": "2025-04-10T04:26:53.785447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A - Implementing and Benchmarking a Custom Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code contains the following functions/classes:<br>\n",
    "    - def zero_shot_captioning(image_path, model_name=\"HuggingFaceTB/SmolVLM-256M-Instruct\")<br>\n",
    "    - def generate_and_save_captions(test_dir, output_csv_path)<br>\n",
    "    - def evaluate_model(test_csv_pathl, generated_csv_path)<br>\n",
    "    - class ImageCaptionDataset(Dataset)<br>\n",
    "    - class ImageCaptionModel(nn.Module)<br>\n",
    "    - def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3, save_path=\"best_custom_model.pth\")<br>\n",
    "    - def generate_captions_with_custom_model(model, image_path, tokenizer, device)<br>\n",
    "    - def generate_captions_for_test_set(model, test_dir, output_csv, device)<br>\n",
    "    - def create_dataloaders(train_csv, val_csv, train_img_dir, val_img_dir, tokenizer, batch_size=8)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:00.943886Z",
     "iopub.status.busy": "2025-04-10T04:27:00.943349Z",
     "iopub.status.idle": "2025-04-10T04:27:00.951368Z",
     "shell.execute_reply": "2025-04-10T04:27:00.950622Z",
     "shell.execute_reply.started": "2025-04-10T04:27:00.943864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot_captioning(image_path, model_name=\"HuggingFaceTB/SmolVLM-256M-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generate captions by Zero Shot on SmolVLM Model\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        model_name (str): Model name (default: HuggingFaceTB/SmolVLM-256M-Instruct).\n",
    "    Returns:\n",
    "        str: Generated caption for the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(zero_shot_captioning, \"model\") or not hasattr(zero_shot_captioning, \"processor\"):\n",
    "        print(\"Loading Model and Processor...\")\n",
    "        zero_shot_captioning.model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            device_map=DEVICE,\n",
    "            _attn_implementation=\"eager\" # here \"flash_attention_2\" could not be used due to cuda version errors on Kaggle Notebook\n",
    "        ).to(DEVICE)\n",
    "        zero_shot_captioning.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        print(\"Model and Processor loaded Successfully!\")\n",
    "        \n",
    "    try:\n",
    "        image = load_image(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Loading image from {image_path} failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = zero_shot_captioning.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = zero_shot_captioning.processor(text=prompt, images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = zero_shot_captioning.model.generate(**inputs, max_new_tokens=100)\n",
    "        generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "        generated_text = zero_shot_captioning.processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "        \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:04.691455Z",
     "iopub.status.busy": "2025-04-10T04:27:04.690765Z",
     "iopub.status.idle": "2025-04-10T04:27:04.696697Z",
     "shell.execute_reply": "2025-04-10T04:27:04.695849Z",
     "shell.execute_reply.started": "2025-04-10T04:27:04.691433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_and_save_captions(test_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Generate captions for all images in the directory and save to a CSV file.\n",
    "    Args:\n",
    "        test_dir (str): Directory containing the images.\n",
    "        output_csv_path (str): Path to save the output CSV file.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing filenames and generated captions.\n",
    "    \"\"\"\n",
    "    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()\n",
    "    \n",
    "    captions = []\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Generating Captions\"):\n",
    "        image_path = os.path.join(test_dir, image_file)\n",
    "        caption = zero_shot_captioning(image_path)\n",
    "        \n",
    "        if caption:\n",
    "            captions.append({\n",
    "                'filename': image_file,\n",
    "                'generated_caption': caption\n",
    "            })\n",
    "            \n",
    "    df = pd.DataFrame(captions)\n",
    "    df.to_csv(output_csv_path, index=True)\n",
    "    print(f\"Captions saved to {output_csv_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:10.338197Z",
     "iopub.status.busy": "2025-04-10T04:27:10.337885Z",
     "iopub.status.idle": "2025-04-10T04:27:10.346275Z",
     "shell.execute_reply": "2025-04-10T04:27:10.345458Z",
     "shell.execute_reply.started": "2025-04-10T04:27:10.338173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(test_csv_pathl, generated_csv_path):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance using the BLEU, ROUGE-L, and METEOR metrics.\n",
    "    Args:\n",
    "        test_csv_path (str): Path to the test CSV file containing ground truth captions.\n",
    "        generated_csv_path (str): Path to the CSV file containing generated captions.\n",
    "    Returns:\n",
    "        dict: Dictionary containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_df = pd.read_csv(test_csv_pathl)\n",
    "    generated_df = pd.read_csv(generated_csv_path)\n",
    "    \n",
    "    merged_df = pd.merge(test_df, generated_df, on='filename', how='inner')\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        print(\"No matching filenames found between test and generated dataframes.\")\n",
    "        return None\n",
    "    \n",
    "    # BLEU Score\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for _, row in merged_df.iterrows():\n",
    "        reference = nltk.word_tokenize(row['caption'].lower())\n",
    "        hypothesis = nltk.word_tokenize(row['generated_caption'].lower())\n",
    "        \n",
    "        references.append([reference])\n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for i in range(len(merged_df)):\n",
    "        score = rouge.score(merged_df.iloc[i]['caption'], merged_df.iloc[i]['generated_caption'])\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)\n",
    "    rouge_l_score = np.mean(rouge_scores)\n",
    "    \n",
    "    # METEOR Score\n",
    "    meteor_scores = []\n",
    "    \n",
    "    for i in range(len(merged_df)):\n",
    "        ref = nltk.word_tokenize(merged_df.iloc[i]['caption'].lower())\n",
    "        hyp = nltk.word_tokenize(merged_df.iloc[i]['generated_caption'].lower())\n",
    "        score = meteor_score([ref], hyp)\n",
    "        meteor_scores.append(score)\n",
    "    meteor_score_avg = np.mean(meteor_scores)\n",
    "    \n",
    "    results = {\n",
    "        'BLEU': bleu_score,\n",
    "        'ROUGE-L': rouge_l_score,\n",
    "        'METEOR': meteor_score_avg\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:14.048682Z",
     "iopub.status.busy": "2025-04-10T04:27:14.048020Z",
     "iopub.status.idle": "2025-04-10T04:27:14.055788Z",
     "shell.execute_reply": "2025-04-10T04:27:14.054952Z",
     "shell.execute_reply.started": "2025-04-10T04:27:14.048662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image captioning model using encoder, decoder architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dir, tokenizer, max_length=50, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx]['filename'])\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224)) # generate a dummy black image if error occurs\n",
    "            \n",
    "        caption_encoding = self.tokenizer(\n",
    "            caption, padding=\"max_length\", \n",
    "            truncation=True, max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        caption_ids = caption_encoding.input_ids.squeeze(0)\n",
    "        \n",
    "        return image, caption_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:18.602784Z",
     "iopub.status.busy": "2025-04-10T04:27:18.602501Z",
     "iopub.status.idle": "2025-04-10T04:27:18.614258Z",
     "shell.execute_reply": "2025-04-10T04:27:18.613512Z",
     "shell.execute_reply.started": "2025-04-10T04:27:18.602764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Image Captioning Model using ViT and GPT2.\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model=\"WinKawaks/vit-small-patch16-224\", gpt2_model=\"gpt2\", freeze_vit=True, freeze_gpt2_partial=True):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "\n",
    "        # setup the encoder\n",
    "        self.encoder = ViTModel.from_pretrained(vit_model)\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        if freeze_vit:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # setup the decoder\n",
    "        gpt2_config = GPT2Config.from_pretrained(gpt2_model)\n",
    "        gpt2_config.add_cross_attention = True # add cross attention layer to the decoder\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model, config=gpt2_config)\n",
    "        self.decoder_dim = self.decoder.config.hidden_size\n",
    "        \n",
    "        if freeze_gpt2_partial:\n",
    "            for i, block in enumerate(self.decoder.transformer.h):\n",
    "                if i < len(self.decoder.transformer.h) - 2:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        self.connect = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.img_token_id = self.tokenizer.convert_tokens_to_ids(\"<|img|>\") if \"<|img|>\" in self.tokenizer.get_vocab() else self.tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "        \n",
    "    def forward(self, images, captions=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image tensor.\n",
    "            captions (torch.Tensor, optional): Input caption tensor. Defaults to None.\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits from the decoder.\n",
    "        \"\"\"\n",
    "        \n",
    "        encoder_outputs = self.encoder(images).last_hidden_state\n",
    "        cls_output = encoder_outputs[:, 0, :]\n",
    "        img_features = self.connect(cls_output)\n",
    "        batch_size = images.size(0)\n",
    "        img_tokens = torch.full((batch_size, 1), self.img_token_id, dtype=torch.long, device=images.device)\n",
    "        \n",
    "        if captions is not None:\n",
    "            if not isinstance(captions, torch.Tensor):\n",
    "                raise ValueError(\"Captions should be a tensor of input_ids.\")\n",
    "            \n",
    "            input_ids = torch.cat([img_tokens, captions], dim=1)\n",
    "            outputs = self.decoder(\n",
    "                input_ids=input_ids,\n",
    "                encoder_hidden_states=img_features.unsqueeze(1),\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            pad_mask = (labels != self.tokenizer.pad_token_id)\n",
    "            \n",
    "            shifted_logits = shifted_logits.view(-1, self.decoder.config.vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "            mask = pad_mask.view(-1)\n",
    "            \n",
    "            shifted_logits = shifted_logits[mask]\n",
    "            labels = labels[mask]\n",
    "            \n",
    "            return shifted_logits, labels\n",
    "        else:\n",
    "            # inference mode\n",
    "            input_ids = img_tokens\n",
    "            encoder_outputs = img_features.unsqueeze(1)\n",
    "            \n",
    "            generated = self.decoder.generate(\n",
    "                input_ids=input_ids,\n",
    "                encoder_hidden_states=encoder_outputs,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "            \n",
    "            return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:25.583015Z",
     "iopub.status.busy": "2025-04-10T04:27:25.582427Z",
     "iopub.status.idle": "2025-04-10T04:27:25.590777Z",
     "shell.execute_reply": "2025-04-10T04:27:25.590015Z",
     "shell.execute_reply.started": "2025-04-10T04:27:25.582956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3, save_path=\"best_custom_model.pth\"):\n",
    "    \"\"\"\n",
    "    Train the image captioning model.\n",
    "    Args:\n",
    "        model (nn.Module): The image captioning model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (str): Device to train on (\"cuda\" or \"cpu\").\n",
    "        epochs (int, optional): Number of epochs to train. Defaults to 3.\n",
    "        save_path (str, optional): Path to save the best model. Defaults to \"best_custom_model.pth\".\n",
    "    \"\"\"\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n",
    "        for images, captions in train_pbar:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            logits, labels = model(images, captions)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, captions in val_pbar:\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                logits, labels = model(images, captions)\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # save the best model to desired path\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved to {save_path} with loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:30.585506Z",
     "iopub.status.busy": "2025-04-10T04:27:30.584736Z",
     "iopub.status.idle": "2025-04-10T04:27:30.592152Z",
     "shell.execute_reply": "2025-04-10T04:27:30.591467Z",
     "shell.execute_reply.started": "2025-04-10T04:27:30.585481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_captions_with_custom_model(model, image_path, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Generate the captions using the trained custom model\n",
    "    Args:\n",
    "        model (nn.Module): The trained image captioning model.\n",
    "        image_path (str): Path to the image file.\n",
    "        device (str): Device to use (\"cuda\" or \"cpu\").\n",
    "    Returns:\n",
    "        str: Generated caption for the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = image.size(0)\n",
    "        input_ids = torch.full((batch_size, 1), model.img_token_id, dtype=torch.long, device=device)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        encoder_outputs = model.encoder(image).last_hidden_state\n",
    "        cls_output = encoder_outputs[:, 0, :]\n",
    "        img_features = model.connect(cls_output).unsqueeze(1)\n",
    "\n",
    "        generated = model.decoder.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=img_features,\n",
    "            max_length=50,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:34.307709Z",
     "iopub.status.busy": "2025-04-10T04:27:34.307161Z",
     "iopub.status.idle": "2025-04-10T04:27:34.312665Z",
     "shell.execute_reply": "2025-04-10T04:27:34.311931Z",
     "shell.execute_reply.started": "2025-04-10T04:27:34.307689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_captions_for_test_set(model, test_dir, output_csv, device):\n",
    "    \"\"\"\n",
    "    Generate the captions using the trained model on the test set images\n",
    "    Args:\n",
    "        model (nn.Module): The trained image captioning model.\n",
    "        test_dir (str): Directory containing the test images.\n",
    "        output_csv (str): Path to save the generated captions.\n",
    "        device (str): Device to use (\"cuda\" or \"cpu\").\n",
    "    \"\"\"\n",
    "    \n",
    "    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()\n",
    "    \n",
    "    captions = []\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Generating Captions\"):\n",
    "        image_path = os.path.join(test_dir, image_file)\n",
    "        caption = generate_captions_with_custom_model(model, image_path, model.tokenizer, device)\n",
    "        \n",
    "        if caption:\n",
    "            captions.append({\n",
    "                'filename': image_file,\n",
    "                'generated_caption': caption\n",
    "            })\n",
    "            \n",
    "    df = pd.DataFrame(captions)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Captions saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:38.493847Z",
     "iopub.status.busy": "2025-04-10T04:27:38.493578Z",
     "iopub.status.idle": "2025-04-10T04:27:38.498402Z",
     "shell.execute_reply": "2025-04-10T04:27:38.497632Z",
     "shell.execute_reply.started": "2025-04-10T04:27:38.493831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(train_csv, val_csv, train_img_dir, val_img_dir, tokenizer, batch_size=8):\n",
    "    train_dataset = ImageCaptionDataset(csv_file=train_csv, img_dir=train_img_dir, tokenizer=tokenizer)\n",
    "    val_dataset = ImageCaptionDataset(csv_file=val_csv, img_dir=val_img_dir, tokenizer=tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the funtion calls and class instances required for Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:44.444877Z",
     "iopub.status.busy": "2025-04-10T04:27:44.444342Z",
     "iopub.status.idle": "2025-04-10T04:27:44.448395Z",
     "shell.execute_reply": "2025-04-10T04:27:44.447610Z",
     "shell.execute_reply.started": "2025-04-10T04:27:44.444855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we will keep the running of this more modular and separated instead of forming a single main, because a single point of failure should not affect more\n",
    "# and also it will be easier to debug and run in parts if needed\n",
    "test_dir = \"/kaggle/input/dataset/Dataset/test\"             # change as required\n",
    "test_csv_path = \"/kaggle/input/dataset/Dataset/test.csv\"    # change as required\n",
    "generated_csv_path = \"smolvlm_captions_0.csv\"               # change as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:27:47.741324Z",
     "iopub.status.busy": "2025-04-10T04:27:47.740827Z",
     "iopub.status.idle": "2025-04-10T05:39:49.669756Z",
     "shell.execute_reply": "2025-04-10T05:39:49.669048Z",
     "shell.execute_reply.started": "2025-04-10T04:27:47.741302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions using the SmolVLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions:   0%|          | 0/928 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model and Processor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5629b5853a4b47a882e5c66a6361cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99907205cce14642b56bac19e64e2a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c184ade6ecf4f22a0e205b4cf7f8cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f717dbc673147c08f64e84ea987419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c870af87f6d1490fa100640cfe9ce0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea0aaf9270f4c0e894fc248bc8f3067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c2e820407a4097b47d7c06d3dcf59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf0b8d43ee146c9ad43e66355969091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b142822e2e4b2ba95e544e61363589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f0270b5c92462c8f6a206bd37f11f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6222c07be52480da7f95faf81366ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a057e4c72ec848369fb832a449707b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Processor loaded Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions: 100%|██████████| 928/928 [1:12:01<00:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to smolvlm_captions_0.csv\n",
      "Generation of captions using SmolVLM model completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating captions using the SmolVLM model...\")\n",
    "generate_and_save_captions(test_dir, generated_csv_path)\n",
    "print(\"Generation of captions using SmolVLM model completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:39:54.096868Z",
     "iopub.status.busy": "2025-04-10T05:39:54.096283Z",
     "iopub.status.idle": "2025-04-10T05:40:05.270023Z",
     "shell.execute_reply": "2025-04-10T05:40:05.269297Z",
     "shell.execute_reply.started": "2025-04-10T05:39:54.096846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the SmolVLM model...\n",
      "\n",
      "Evaluation Results:\n",
      "BLEU Score: 0.0545\n",
      "ROUGE-L Score: 0.2396\n",
      "METEOR Score: 0.2750\n",
      "Evaluation of SmolVLM model completed and results saved to smolvlm_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the SmolVLM model...\")\n",
    "evaluation_results = evaluate_model(test_csv_path, generated_csv_path)\n",
    "if evaluation_results:\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"BLEU Score: {evaluation_results['BLEU']:.4f}\")\n",
    "    print(f\"ROUGE-L Score: {evaluation_results['ROUGE-L']:.4f}\")\n",
    "    print(f\"METEOR Score: {evaluation_results['METEOR']:.4f}\")\n",
    "df_results_smolvlm = pd.DataFrame([evaluation_results])\n",
    "df_results_smolvlm.to_csv(\"smolvlm_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation of SmolVLM model completed and results saved to smolvlm_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:40:10.013327Z",
     "iopub.status.busy": "2025-04-10T05:40:10.013064Z",
     "iopub.status.idle": "2025-04-10T05:40:10.017489Z",
     "shell.execute_reply": "2025-04-10T05:40:10.016726Z",
     "shell.execute_reply.started": "2025-04-10T05:40:10.013307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_csv = \"/kaggle/input/dataset/Dataset/train.csv\"       # change as required\n",
    "val_csv = \"/kaggle/input/dataset/Dataset/val.csv\"           # change as required\n",
    "test_csv = \"/kaggle/input/dataset/Dataset/test.csv\"         # change as required\n",
    "train_img_dir = \"/kaggle/input/dataset/Dataset/train\"       # change as required\n",
    "val_img_dir = \"/kaggle/input/dataset/Dataset/val\"           # change as required\n",
    "test_img_dir = \"/kaggle/input/dataset/Dataset/test\"         # change as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:40:12.711090Z",
     "iopub.status.busy": "2025-04-10T05:40:12.710412Z",
     "iopub.status.idle": "2025-04-10T05:40:19.351158Z",
     "shell.execute_reply": "2025-04-10T05:40:19.350324Z",
     "shell.execute_reply.started": "2025-04-10T05:40:12.711060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f25dfa9cfe4398bab9121f2af58a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a89b29d60a4278a49440ce5dc61590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e008f3d51cf47fea92ed66c9d0aa2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d97bb5e2bf40beb48f36a01316f7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ab0830eb7e4fd78a70d924a8dcac8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa09a7afd49440529e52307f1fce2222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf7ecdf819445de890d4fac5bb87b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8640da4d5de4931afb0f59f1c59a624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6444ba6ba52d4818bc1852f1157100b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ImageCaptionModel()\n",
    "tokenizer = model.tokenizer\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:40:28.936206Z",
     "iopub.status.busy": "2025-04-10T05:40:28.935478Z",
     "iopub.status.idle": "2025-04-10T05:40:28.992620Z",
     "shell.execute_reply": "2025-04-10T05:40:28.991853Z",
     "shell.execute_reply.started": "2025-04-10T05:40:28.936182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_csv=train_csv,\n",
    "    val_csv=val_csv,\n",
    "    train_img_dir=train_img_dir,\n",
    "    val_img_dir=val_img_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8            # adjust according to the GPU availability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:40:35.003627Z",
     "iopub.status.busy": "2025-04-10T05:40:35.003206Z",
     "iopub.status.idle": "2025-04-10T05:50:10.580831Z",
     "shell.execute_reply": "2025-04-10T05:50:10.580066Z",
     "shell.execute_reply.started": "2025-04-10T05:40:35.003601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]: 100%|██████████| 715/715 [02:11<00:00,  5.43it/s, loss=2.82]\n",
      "Epoch 1/5 [Validation]: 100%|██████████| 119/119 [00:16<00:00,  7.42it/s, loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 2.7448, Val Loss: 2.4590\n",
      "Best model saved to best_custom_model.pth with loss: 2.4590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Training]: 100%|██████████| 715/715 [01:36<00:00,  7.44it/s, loss=2.8] \n",
      "Epoch 2/5 [Validation]: 100%|██████████| 119/119 [00:09<00:00, 11.94it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 2.4296, Val Loss: 2.4042\n",
      "Best model saved to best_custom_model.pth with loss: 2.4042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Training]: 100%|██████████| 715/715 [01:35<00:00,  7.47it/s, loss=2.33]\n",
      "Epoch 3/5 [Validation]: 100%|██████████| 119/119 [00:10<00:00, 11.67it/s, loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 2.2758, Val Loss: 2.3820\n",
      "Best model saved to best_custom_model.pth with loss: 2.3820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Training]: 100%|██████████| 715/715 [01:35<00:00,  7.48it/s, loss=2.19]\n",
      "Epoch 4/5 [Validation]: 100%|██████████| 119/119 [00:10<00:00, 11.88it/s, loss=2.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 2.1526, Val Loss: 2.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Training]: 100%|██████████| 715/715 [01:35<00:00,  7.47it/s, loss=2]   \n",
      "Epoch 5/5 [Validation]: 100%|██████████| 119/119 [00:09<00:00, 11.96it/s, loss=2.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 2.0431, Val Loss: 2.4055\n",
      "Model Training Completed and Best Model Saved to best_custom_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion  = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "print(\"Starting Model Training...\")\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    epochs=5,               # change as required\n",
    "    save_path=\"best_custom_model.pth\"\n",
    ")\n",
    "print(\"Model Training Completed and Best Model Saved to best_custom_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:55:20.863607Z",
     "iopub.status.busy": "2025-04-10T05:55:20.863044Z",
     "iopub.status.idle": "2025-04-10T05:55:21.308405Z",
     "shell.execute_reply": "2025-04-10T05:55:21.307742Z",
     "shell.execute_reply.started": "2025-04-10T05:55:20.863584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model for inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/842038368.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_custom_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading the best model for inference...\")\n",
    "model.load_state_dict(torch.load(\"best_custom_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:55:31.213391Z",
     "iopub.status.busy": "2025-04-10T05:55:31.213120Z",
     "iopub.status.idle": "2025-04-10T06:06:57.270580Z",
     "shell.execute_reply": "2025-04-10T06:06:57.269822Z",
     "shell.execute_reply.started": "2025-04-10T05:55:31.213372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Captions for the test set using the trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions: 100%|██████████| 928/928 [11:26<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to custom_model_captions_0.csv\n",
      "Captions for the test set generated and saved to custom_model_captions_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Captions for the test set using the trained model...\")\n",
    "generate_captions_for_test_set(\n",
    "    model=model,\n",
    "    test_dir=test_img_dir,\n",
    "    output_csv=\"custom_model_captions_0.csv\",\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"Captions for the test set generated and saved to custom_model_captions_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:08:58.707799Z",
     "iopub.status.busy": "2025-04-10T06:08:58.707162Z",
     "iopub.status.idle": "2025-04-10T06:09:03.466140Z",
     "shell.execute_reply": "2025-04-10T06:09:03.465390Z",
     "shell.execute_reply.started": "2025-04-10T06:08:58.707770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the custom model performance...\n",
      "\n",
      "Evaluation Results:\n",
      "BLEU Score: 0.0592\n",
      "ROUGE-L Score: 0.2722\n",
      "METEOR Score: 0.2156\n",
      "Evaluation of custom model completed and results saved to custom_model_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the custom model performance...\")\n",
    "evaluation_results = evaluate_model(test_csv, \"custom_model_captions_0.csv\")\n",
    "if evaluation_results:\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"BLEU Score: {evaluation_results['BLEU']:.4f}\")\n",
    "    print(f\"ROUGE-L Score: {evaluation_results['ROUGE-L']:.4f}\")\n",
    "    print(f\"METEOR Score: {evaluation_results['METEOR']:.4f}\")\n",
    "df = pd.DataFrame([evaluation_results])\n",
    "df.to_csv(\"custom_model_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation of custom model completed and results saved to custom_model_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B - Studying Performance Change Under Image Occlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code contains the following functions/classes:<br>\n",
    "    - def occlude_image(image, mask_percentage)<br>\n",
    "    - def generate_smolvlm_captions(test_dir, occlusion_levels, output_path_template=\"smolvlm_captions_{}.csv\", model_name=\"HuggingFaceTB/SmolVLM-256M-Instruct\")<br>\n",
    "    - def generate_custom_model_captions(test_dir, occlusion_levels, model_path, output_path_template=\"custom_model_captions_{}.csv\")<br>\n",
    "    - def evaluate_on_occluded_images(model_name, test_csv_path, baseline_csv_path, occluded_captions_template, occlusion_levels)<br>\n",
    "    - def evaluate_and_save_results(test_csv_path, generated_dir, output_csv_path, partc_csv_path)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:14.898368Z",
     "iopub.status.busy": "2025-04-10T06:09:14.897859Z",
     "iopub.status.idle": "2025-04-10T06:09:14.903668Z",
     "shell.execute_reply": "2025-04-10T06:09:14.902834Z",
     "shell.execute_reply.started": "2025-04-10T06:09:14.898336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def occlude_image(image, mask_percentage):\n",
    "    \"\"\"\n",
    "    Apply patch waise occlusion to the given image\n",
    "    Args:\n",
    "        image (np.array): Input image array.\n",
    "        mask_percentage (float): Percentage of the image to be occluded\n",
    "    Returns:\n",
    "        np.array: Occluded image array.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(image, Image.Image):\n",
    "        np_image = np.array(image)\n",
    "    else:\n",
    "        np_image = image.copy()\n",
    "        \n",
    "    height, width, _ = np_image.shape\n",
    "    \n",
    "    patch_size = 16 # as mentioned in the assignment\n",
    "    \n",
    "    patches_h = height // patch_size\n",
    "    patches_w = width // patch_size\n",
    "    total_patches = patches_h * patches_w\n",
    "    \n",
    "    num_patches_to_mask = int((mask_percentage / 100) * total_patches)\n",
    "    mask_indices = random.sample(range(total_patches), num_patches_to_mask)\n",
    "    \n",
    "    for idx in mask_indices:\n",
    "        patch_h = idx // patches_w\n",
    "        patch_w = idx % patches_w\n",
    "        \n",
    "        h_start = patch_h * patch_size\n",
    "        h_end = min(h_start + patch_size, height)\n",
    "        w_start = patch_w * patch_size\n",
    "        w_end = min(w_start + patch_size, width)\n",
    "        \n",
    "        np_image[h_start:h_end, w_start:w_end, :] = 0 # set the patch to black\n",
    "        \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:18.254811Z",
     "iopub.status.busy": "2025-04-10T06:09:18.254137Z",
     "iopub.status.idle": "2025-04-10T06:09:18.263087Z",
     "shell.execute_reply": "2025-04-10T06:09:18.262444Z",
     "shell.execute_reply.started": "2025-04-10T06:09:18.254790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_smolvlm_captions(test_dir, occlusion_levels, output_path_template=\"smolvlm_captions_{}.csv\", model_name=\"HuggingFaceTB/SmolVLM-256M-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generate captions using the SmolVLM model on the given different levels of occlusion\n",
    "    Args:\n",
    "        test_dir (str): Directory containing the images.\n",
    "        occlusion_levels (list): List of occlusion levels (in percentage).\n",
    "        output_path_template (str): Template for output CSV file names.\n",
    "        model_name (str): Model name (default: HuggingFaceTB/SmolVLM-256M-Instruct).\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    filenames.sort()\n",
    "    \n",
    "    print(\"Loading SmolVLm Model and Processor...\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        device_map=DEVICE,\n",
    "        _attn_implementation=\"eager\" # here \"flash_attention_2\" could not be used due to cuda version errors on Kaggle Notebook\n",
    "    ).to(DEVICE)\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    print(\"Model and Processor loaded Successfully!\")\n",
    "    \n",
    "    for occlusion_level in occlusion_levels:\n",
    "        print(f\"ProcessingOcclusion Level: {occlusion_level}%\")\n",
    "        output_path = output_path_template.format(occlusion_level)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for filename in tqdm(filenames, desc=f\"Generating captions for {occlusion_level}% occlusion\"):\n",
    "            try:\n",
    "                image_path = os.path.join(test_dir, filename)\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                occluded_image = Image.fromarray(occlude_image(image, occlusion_level))\n",
    "                \n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\" : [\n",
    "                            {\"type\": \"image\"},\n",
    "                            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "                inputs = processor(text=prompt, images=occluded_image, return_tensors=\"pt\").to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "                    generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "                    generated_text = processor.batch_decode(\n",
    "                        generated_ids, skip_special_tokens=True\n",
    "                    )[0].strip()\n",
    "                    \n",
    "                    captions.append({\n",
    "                        'filename': filename,\n",
    "                        'generated_caption': generated_text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {filename}: {e}\")\n",
    "                captions.append({\n",
    "                    'filename': filename,\n",
    "                    'generated_caption': \"\"\n",
    "                })\n",
    "                \n",
    "        df = pd.DataFrame(captions)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Captions saved to {output_path}\")\n",
    "        print(f\"Captions for {occlusion_level}% occlusion completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:21.238259Z",
     "iopub.status.busy": "2025-04-10T06:09:21.237744Z",
     "iopub.status.idle": "2025-04-10T06:09:21.247308Z",
     "shell.execute_reply": "2025-04-10T06:09:21.246475Z",
     "shell.execute_reply.started": "2025-04-10T06:09:21.238236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_custom_model_captions(test_dir, occlusion_levels, model_path, output_path_template=\"custom_model_captions_{}.csv\"):\n",
    "    \"\"\"\n",
    "    Generate captions using the custom model on the given different levels of occlusion\n",
    "    Args:\n",
    "        test_dir (str): Directory containing the images.\n",
    "        occlusion_levels (list): List of occlusion levels (in percentage).\n",
    "        model_path (str): Path to the trained custom model.\n",
    "        output_path_template (str): Template for output CSV file names.\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    filenames.sort()\n",
    "    \n",
    "    print(\"Loading the trained custom model...\")\n",
    "    model = ImageCaptionModel().to(DEVICE)\n",
    "    use_weights_only = torch.__version__ >= \"2.3\" # for compatibility with torch 2.3 and above\n",
    "    state_dict = torch.load(model_path, map_location=DEVICE, weights_only=use_weights_only)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model.eval()    # set the model to evaluation mode\n",
    "    \n",
    "    # create the pad token if not already present or if it is same as eos token\n",
    "    # this is important for the model to work properly in inference mode\n",
    "    if model.tokenizer.pad_token is None or model.tokenizer.pad_token == model.tokenizer.eos_token:\n",
    "        model.tokenizer.pad_token = \"[PAD]\"\n",
    "        \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    for occlusion_level in occlusion_levels:\n",
    "        print(f\"Generating Captions for Occlusion Level: {occlusion_level}%\")\n",
    "        output_path = output_path_template.format(occlusion_level)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for filename in tqdm(filenames, desc=f\"Generating Captions for {occlusion_level}% occlusion\"):\n",
    "            try:\n",
    "                image_path = os.path.join(test_dir, filename)\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                occluded_image = Image.fromarray(occlude_image(image, occlusion_level))\n",
    "                \n",
    "                image_tensor = preprocess(occluded_image).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model(image_tensor)\n",
    "                    generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                    \n",
    "                captions.append({\n",
    "                    'filename': filename,\n",
    "                    'generated_caption': generated_text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {filename}: {e}\")\n",
    "                captions.append({\n",
    "                    'filename': filename,\n",
    "                    'generated_caption': \"\"\n",
    "                })\n",
    "                \n",
    "            if len(captions) % 100 == 0:\n",
    "                temp_df = pd.DataFrame(captions)\n",
    "                temp_path = f\"temp_{output_path}\"\n",
    "                temp_df.to_csv(temp_path, index=False)\n",
    "                print(f\"Saved intermediate results to {temp_path} ({len(captions)} / {len(filenames)})\")\n",
    "                \n",
    "        df = pd.DataFrame(captions)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Captions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:24.498185Z",
     "iopub.status.busy": "2025-04-10T06:09:24.497538Z",
     "iopub.status.idle": "2025-04-10T06:09:24.503345Z",
     "shell.execute_reply": "2025-04-10T06:09:24.502549Z",
     "shell.execute_reply.started": "2025-04-10T06:09:24.498163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_on_occluded_images(model_name, test_csv_path, baseline_csv_path, occluded_captions_template, occlusion_levels):\n",
    "    \"\"\"\n",
    "    Evaluate after occluding images\n",
    "    Args:\n",
    "        model_name (str): Name of the model used for evaluation.\n",
    "        test_csv_path (str): Path to the test CSV file containing ground truth captions.\n",
    "        baseline_csv_path (str): Path to the CSV file containing baseline captions.\n",
    "        occluded_captions_template (str): Template for occluded captions CSV file names.\n",
    "        occlusion_levels (list): List of occlusion levels (in percentage).\n",
    "    Returns:\n",
    "        dict: Dictionary containing the evaluation metrics for each occlusion level.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Evaluating {model_name} Baseline performance (0 occlusion)\")\n",
    "    baseline_metrics = evaluate_model(test_csv_path, baseline_csv_path)\n",
    "    \n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"occlusion_results\": {}\n",
    "    }\n",
    "    \n",
    "    for occlusion_level in occlusion_levels:\n",
    "        print(f\"Evaluating {model_name} for {occlusion_level}% Occlusion...\")\n",
    "        occluded_csv_path = occluded_captions_template.format(occlusion_level)\n",
    "        \n",
    "        occluded_metrics = evaluate_model(test_csv_path, occluded_csv_path)\n",
    "        \n",
    "        changes = {\n",
    "            \"BLEU_change\": occluded_metrics[\"BLEU\"] - baseline_metrics[\"BLEU\"],\n",
    "            \"ROUGE-L_change\": occluded_metrics[\"ROUGE-L\"] - baseline_metrics[\"ROUGE-L\"],\n",
    "            \"METEOR_change\": occluded_metrics[\"METEOR\"] - baseline_metrics[\"METEOR\"]\n",
    "        }\n",
    "        \n",
    "        results[\"occlusion_results\"][occlusion_level] = {\n",
    "            \"metrics\": occluded_metrics,\n",
    "            \"changes\": changes\n",
    "        }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:29:19.387168Z",
     "iopub.status.busy": "2025-04-10T10:29:19.386897Z",
     "iopub.status.idle": "2025-04-10T10:29:19.398494Z",
     "shell.execute_reply": "2025-04-10T10:29:19.397636Z",
     "shell.execute_reply.started": "2025-04-10T10:29:19.387151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(test_csv_path, generated_dir, output_csv_path, partc_csv_path):\n",
    "    \"\"\"\n",
    "    Evaluate model against the 0 occlusion level and save the obtaained results\n",
    "    Args:\n",
    "        test_csv_path (str): Path to the test CSV file containing ground truth captions.\n",
    "        generated_dir (str): Directory containing the generated captions.\n",
    "        output_csv_path (str): Path to save the evaluation results.\n",
    "        partc_csv_path (str): Path to save the participant CSV file.\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    results_list = []\n",
    "    partc_list = []\n",
    "    \n",
    "    occlusion_levels = [0, 10, 50, 80]\n",
    "    models = [\"custom_model\", \"smolvlm\"]\n",
    "    \n",
    "    baseline_scores = {}\n",
    "    \n",
    "    for model in models:\n",
    "        for occlusion_level in occlusion_levels:\n",
    "            generated_csv_path = os.path.join(generated_dir, f\"{model}_captions_{occlusion_level}.csv\")\n",
    "            \n",
    "            if not os.path.exists(generated_csv_path):\n",
    "                print(f\"File not found: {generated_csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            generated_df = pd.read_csv(generated_csv_path)\n",
    "            merged_df = pd.merge(test_df, generated_df, on='filename', how='inner')\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(f\"No matching filenames found between test and generated dataframes for {model} at {occlusion_level}% occlusion.\")\n",
    "                continue\n",
    "            \n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            \n",
    "            for _, row in merged_df.iterrows():\n",
    "                reference = nltk.word_tokenize(row['caption'].lower())\n",
    "                hypothesis = nltk.word_tokenize(row['generated_caption'].lower())\n",
    "                \n",
    "                references.append([reference])\n",
    "                hypotheses.append(hypothesis)\n",
    "                \n",
    "                partc_list.append([\n",
    "                    row['caption'], row['generated_caption'], occlusion_level, model\n",
    "                ])\n",
    "                \n",
    "            # BLEU Score\n",
    "            smooth = SmoothingFunction().method1\n",
    "            bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "            \n",
    "            # ROUGE-L Score\n",
    "            rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "            rouge_scores = [\n",
    "                rouge.score(row['caption'], row['generated_caption'])['rougeL'].fmeasure\n",
    "                for _, row in merged_df.iterrows()\n",
    "            ]\n",
    "            rouge_l_score = np.mean(rouge_scores)\n",
    "            \n",
    "            # METEOR Score\n",
    "            meteor_scores = [\n",
    "                meteor_score([nltk.word_tokenize(row['caption'].lower())],\n",
    "                             nltk.word_tokenize(row['generated_caption'].lower())) \n",
    "                for _, row in merged_df.iterrows()\n",
    "            ]\n",
    "            meteor_score_avg = np.mean(meteor_scores)\n",
    "            \n",
    "            if occlusion_level == 0:\n",
    "                baseline_scores[model] = {\n",
    "                    'BLEU': bleu_score,\n",
    "                    'ROUGE-L': rouge_l_score,\n",
    "                    'METEOR': meteor_score_avg\n",
    "                }\n",
    "                \n",
    "            differences = {\n",
    "                \"BLEU_diff\": baseline_scores[model][\"BLEU\"] - bleu_score if occlusion_level != 0 else 0,\n",
    "                \"ROUGE-L_diff\": baseline_scores[model][\"ROUGE-L\"] - rouge_l_score if occlusion_level != 0 else 0,\n",
    "                \"METEOR_diff\": baseline_scores[model][\"METEOR\"] - meteor_score_avg if occlusion_level != 0 else 0,\n",
    "            }\n",
    "            \n",
    "            results_list.append([\n",
    "                model, occlusion_level, bleu_score, rouge_l_score, meteor_score_avg,\n",
    "                differences[\"BLEU_diff\"], differences[\"ROUGE-L_diff\"], differences[\"METEOR_diff\"]\n",
    "            ])\n",
    "            \n",
    "        results_df = pd.DataFrame(results_list, columns=[\n",
    "            \"Model\", \"Occlusion Level\", \"BLEU\", \"ROUGE-L\", \"METEOR\", \n",
    "            \"BLEU Difference\", \"ROUGE-L Difference\", \"METEOR Difference\"\n",
    "        ])\n",
    "        results_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "        \n",
    "        # for further Part C of the assignment\n",
    "        partc_df = pd.DataFrame(partc_list, columns=[\"original_caption\", \"generated_caption\", \"occlusion_level\", \"model\"])\n",
    "        partc_df.to_csv(partc_csv_path, index=False)\n",
    "        print(f\"Part C required csv saved to {partc_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the funtion calls and class instances required for Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:55.434963Z",
     "iopub.status.busy": "2025-04-10T06:09:55.434404Z",
     "iopub.status.idle": "2025-04-10T06:09:55.438457Z",
     "shell.execute_reply": "2025-04-10T06:09:55.437551Z",
     "shell.execute_reply.started": "2025-04-10T06:09:55.434942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we will keep the running of this more modular and separated instead of forming a single main, because a single point of failure should not affect more\n",
    "# and also it will be easier to debug and run in parts if needed\n",
    "test_dir = \"/kaggle/input/dataset/Dataset/test\"             # change as required\n",
    "test_csv_path = \"/kaggle/input/dataset/Dataset/test.csv\"    # change as required\n",
    "model_path = \"/kaggle/working/best_custom_model.pth\"        # change as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:09:57.775782Z",
     "iopub.status.busy": "2025-04-10T06:09:57.775510Z",
     "iopub.status.idle": "2025-04-10T06:09:57.779661Z",
     "shell.execute_reply": "2025-04-10T06:09:57.778963Z",
     "shell.execute_reply.started": "2025-04-10T06:09:57.775763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "occlusion_levels = [10, 50, 80] # as mentioned in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:10:00.786737Z",
     "iopub.status.busy": "2025-04-10T06:10:00.786459Z",
     "iopub.status.idle": "2025-04-10T09:44:59.062777Z",
     "shell.execute_reply": "2025-04-10T09:44:59.061916Z",
     "shell.execute_reply.started": "2025-04-10T06:10:00.786716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SmolVLm Model and Processor...\n",
      "Model and Processor loaded Successfully!\n",
      "ProcessingOcclusion Level: 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions for 10% occlusion: 100%|██████████| 928/928 [1:12:03<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to smolvlm_captions_10.csv\n",
      "Captions for 10% occlusion completed successfully!\n",
      "ProcessingOcclusion Level: 50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions for 50% occlusion: 100%|██████████| 928/928 [1:11:16<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to smolvlm_captions_50.csv\n",
      "Captions for 50% occlusion completed successfully!\n",
      "ProcessingOcclusion Level: 80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions for 80% occlusion: 100%|██████████| 928/928 [1:11:35<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to smolvlm_captions_80.csv\n",
      "Captions for 80% occlusion completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_smolvlm_captions(test_dir, occlusion_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:47:17.293042Z",
     "iopub.status.busy": "2025-04-10T09:47:17.292739Z",
     "iopub.status.idle": "2025-04-10T10:21:38.291027Z",
     "shell.execute_reply": "2025-04-10T10:21:38.290249Z",
     "shell.execute_reply.started": "2025-04-10T09:47:17.293018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained custom model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Captions for Occlusion Level: 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  11%|█         | 100/928 [01:13<10:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (100 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  22%|██▏       | 200/928 [02:27<09:03,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (200 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  32%|███▏      | 300/928 [03:41<07:46,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (300 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  43%|████▎     | 400/928 [04:55<06:32,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (400 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  54%|█████▍    | 500/928 [06:09<05:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (500 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  65%|██████▍   | 600/928 [07:23<04:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (600 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  75%|███████▌  | 700/928 [08:37<02:49,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (700 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  86%|████████▌ | 800/928 [09:51<01:35,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (800 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion:  97%|█████████▋| 900/928 [11:05<00:20,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_10.csv (900 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 10% occlusion: 100%|██████████| 928/928 [11:26<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to custom_model_captions_10.csv\n",
      "Generating Captions for Occlusion Level: 50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  11%|█         | 100/928 [01:13<10:07,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (100 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  22%|██▏       | 200/928 [02:27<09:07,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (200 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  32%|███▏      | 300/928 [03:41<07:46,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (300 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  43%|████▎     | 400/928 [04:55<06:28,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (400 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  54%|█████▍    | 500/928 [06:09<05:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (500 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  65%|██████▍   | 600/928 [07:23<04:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (600 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  75%|███████▌  | 700/928 [08:37<02:48,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (700 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  86%|████████▌ | 800/928 [09:51<01:35,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (800 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion:  97%|█████████▋| 900/928 [11:05<00:20,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_50.csv (900 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 50% occlusion: 100%|██████████| 928/928 [11:25<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to custom_model_captions_50.csv\n",
      "Generating Captions for Occlusion Level: 80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  11%|█         | 100/928 [01:14<10:16,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (100 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  22%|██▏       | 200/928 [02:27<08:56,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (200 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  32%|███▏      | 300/928 [03:42<07:43,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (300 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  43%|████▎     | 400/928 [04:56<06:30,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (400 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  54%|█████▍    | 500/928 [06:10<05:16,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (500 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  65%|██████▍   | 600/928 [07:24<04:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (600 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  75%|███████▌  | 700/928 [08:38<02:48,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (700 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  86%|████████▌ | 800/928 [09:52<01:36,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (800 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion:  97%|█████████▋| 900/928 [11:06<00:20,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to temp_custom_model_captions_80.csv (900 / 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions for 80% occlusion: 100%|██████████| 928/928 [11:27<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to custom_model_captions_80.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_custom_model_captions(test_dir, occlusion_levels, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:29:25.665564Z",
     "iopub.status.busy": "2025-04-10T10:29:25.664897Z",
     "iopub.status.idle": "2025-04-10T10:30:14.552611Z",
     "shell.execute_reply": "2025-04-10T10:30:14.551547Z",
     "shell.execute_reply.started": "2025-04-10T10:29:25.665534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /kaggle/working/overall_results.csv\n",
      "Part C required csv saved to /kaggle/working/partc.csv\n",
      "Results saved to /kaggle/working/overall_results.csv\n",
      "Part C required csv saved to /kaggle/working/partc.csv\n"
     ]
    }
   ],
   "source": [
    "# the paths have been given wrt Kaggle Notebook, change as required\n",
    "evaluate_and_save_results(\"/kaggle/input/dataset/Dataset/test.csv\", \"/kaggle/working/\", \"/kaggle/working/overall_results.csv\", \"/kaggle/working/partc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:31:52.208846Z",
     "iopub.status.busy": "2025-04-10T10:31:52.208411Z",
     "iopub.status.idle": "2025-04-10T10:31:52.221763Z",
     "shell.execute_reply": "2025-04-10T10:31:52.220951Z",
     "shell.execute_reply.started": "2025-04-10T10:31:52.208826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame shape: (928, 3)\n"
     ]
    }
   ],
   "source": [
    "# FOR BERT SCORE ANALYSIS\n",
    "test_df = pd.read_csv('/kaggle/input/dataset/Dataset/test.csv') # change as required\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "occlusion_levels = [0, 10, 50, 80]\n",
    "results = {\n",
    "    'model' : [],\n",
    "    'occlusion_level' : [],\n",
    "    'precision' : [],\n",
    "    'recall' : [],\n",
    "    'f1' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate section to calculate the BERT score for all the captions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:31:56.120237Z",
     "iopub.status.busy": "2025-04-10T10:31:56.119953Z",
     "iopub.status.idle": "2025-04-10T10:31:56.124647Z",
     "shell.execute_reply": "2025-04-10T10:31:56.123948Z",
     "shell.execute_reply.started": "2025-04-10T10:31:56.120215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_bert_score(references, candidates, model_type='bert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Calculate the BERT score between the reference and candidate sentences.\n",
    "    Args:\n",
    "        reference (str): Reference sentence.\n",
    "        candidates (list): List of candidate sentences.\n",
    "        model_type (str): BERT model type. Defaults to 'bert-base-uncased'.\n",
    "    Returns:\n",
    "        tuple: Precision, Recall, F1 score.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(candidates, references, lang='en', model_type=model_type, device=DEVICE)\n",
    "    return P.numpy(), R.numpy(), F1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:33:52.429874Z",
     "iopub.status.busy": "2025-04-10T10:33:52.429389Z",
     "iopub.status.idle": "2025-04-10T10:34:43.062710Z",
     "shell.execute_reply": "2025-04-10T10:34:43.062030Z",
     "shell.execute_reply.started": "2025-04-10T10:33:52.429849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SmolVLM captions with occlusion level 0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6064dc0a61d44f709185f4de81420e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d1efbe1e034623a81b02884ba5cf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17bd3947ed24009a2461b7b781ffd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994eaa0285c4ddfb0a82444427dc21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4d95229aab4043acb92bb536032e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4970172345638275, Recall: 0.5353716015815735, F1: 0.5149171948432922\n",
      "Processing Custom Model captions with occlusion level 0%\n",
      "Precision: 0.561847448348999, Recall: 0.5097543001174927, F1: 0.5337085127830505\n",
      "Processing SmolVLM captions with occlusion level 10%\n",
      "Precision: 0.49908581376075745, Recall: 0.5365116000175476, F1: 0.5165361166000366\n",
      "Processing Custom Model captions with occlusion level 10%\n",
      "Precision: 0.56614750623703, Recall: 0.5117161870002747, F1: 0.5367445945739746\n",
      "Processing SmolVLM captions with occlusion level 50%\n",
      "Precision: 0.4868716597557068, Recall: 0.5268229842185974, F1: 0.5054410696029663\n",
      "Processing Custom Model captions with occlusion level 50%\n",
      "Precision: 0.5678975582122803, Recall: 0.5069512128829956, F1: 0.5346135497093201\n",
      "Processing SmolVLM captions with occlusion level 80%\n",
      "Precision: 0.4533844590187073, Recall: 0.4975966513156891, F1: 0.4738430380821228\n",
      "Processing Custom Model captions with occlusion level 80%\n",
      "Precision: 0.5716530084609985, Recall: 0.5055425763130188, F1: 0.5356932282447815\n"
     ]
    }
   ],
   "source": [
    "for ocl in occlusion_levels:\n",
    "    smolvlm_captions = f\"smolvlm_captions_{ocl}.csv\" # change as required\n",
    "    custom_model_captions = f\"custom_model_captions_{ocl}.csv\" # change as required\n",
    "    \n",
    "    if os.path.exists(smolvlm_captions):\n",
    "        smolvlm_df = pd.read_csv(smolvlm_captions)\n",
    "        print(f\"Processing SmolVLM captions with occlusion level {ocl}%\")\n",
    "        \n",
    "        references = test_df['caption'].tolist()\n",
    "        candidates = smolvlm_df['generated_caption'].tolist()\n",
    "        \n",
    "        precision, recall, f1 = calculate_bert_score(references, candidates)\n",
    "        \n",
    "        results['model'].append('SmolVLM')\n",
    "        results['occlusion_level'].append(ocl)\n",
    "        results['precision'].append(np.mean(precision))\n",
    "        results['recall'].append(np.mean(recall))\n",
    "        results['f1'].append(np.mean(f1))\n",
    "        \n",
    "        print(f\"Precision: {np.mean(precision)}, Recall: {np.mean(recall)}, F1: {np.mean(f1)}\")\n",
    "    else:\n",
    "        print(f\"File not found: {smolvlm_captions}\")\n",
    "        \n",
    "    \n",
    "    if os.path.exists(custom_model_captions):\n",
    "        custom_model_df = pd.read_csv(custom_model_captions)\n",
    "        print(f\"Processing Custom Model captions with occlusion level {ocl}%\")\n",
    "        \n",
    "        references = test_df['caption'].tolist()\n",
    "        candidates = custom_model_df['generated_caption'].tolist()\n",
    "        \n",
    "        precision, recall, f1 = calculate_bert_score(references, candidates)\n",
    "        \n",
    "        results['model'].append('Custom Model')\n",
    "        results['occlusion_level'].append(ocl)\n",
    "        results['precision'].append(np.mean(precision))\n",
    "        results['recall'].append(np.mean(recall))\n",
    "        results['f1'].append(np.mean(f1))\n",
    "        \n",
    "        print(f\"Precision: {np.mean(precision)}, Recall: {np.mean(recall)}, F1: {np.mean(f1)}\")\n",
    "    else:\n",
    "        print(f\"File not found: {custom_model_captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:34:58.678295Z",
     "iopub.status.busy": "2025-04-10T10:34:58.678044Z",
     "iopub.status.idle": "2025-04-10T10:34:58.699298Z",
     "shell.execute_reply": "2025-04-10T10:34:58.698625Z",
     "shell.execute_reply.started": "2025-04-10T10:34:58.678278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score results saved to bert_score_results.csv\n",
      "BERT Score Summary:\n",
      "          model  occlusion_level  precision    recall        f1\n",
      "0  Custom Model                0   0.561847  0.509754  0.533709\n",
      "1  Custom Model               10   0.566148  0.511716  0.536745\n",
      "2  Custom Model               50   0.567898  0.506951  0.534614\n",
      "3  Custom Model               80   0.571653  0.505543  0.535693\n",
      "4       SmolVLM                0   0.497017  0.535372  0.514917\n",
      "5       SmolVLM               10   0.499086  0.536512  0.516536\n",
      "6       SmolVLM               50   0.486872  0.526823  0.505441\n",
      "7       SmolVLM               80   0.453384  0.497597  0.473843\n",
      "BERT Score Analysis Completed!\n"
     ]
    }
   ],
   "source": [
    "bert_results_df = pd.DataFrame(results)\n",
    "bert_results_df.to_csv(\"bert_score_results.csv\", index=False)\n",
    "print(\"BERT Score results saved to bert_score_results.csv\")\n",
    "print(\"BERT Score Summary:\")\n",
    "print(bert_results_df.groupby(['model', 'occlusion_level']).agg({'precision': 'mean', 'recall': 'mean', 'f1': 'mean'}).reset_index())\n",
    "print(\"BERT Score Analysis Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C - Building a BERT-based Classifier for Model Identification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code contains the following functions/classes:<br>\n",
    "    - class CaptionClassifier(nn.Module)<br>\n",
    "    - class CaptionDataset(Dataset)<br>\n",
    "    - def train_classifier(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=10)<br>\n",
    "    - def evaluate_classifier(model, test_dataloader, device)<br>\n",
    "    - def run_part_c()<br>\n",
    "    - def prepare_dataframe(df)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:35:24.648312Z",
     "iopub.status.busy": "2025-04-10T10:35:24.648047Z",
     "iopub.status.idle": "2025-04-10T10:35:24.704140Z",
     "shell.execute_reply": "2025-04-10T10:35:24.703479Z",
     "shell.execute_reply.started": "2025-04-10T10:35:24.648291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>occlusion_level</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A large building with bars on the windows in f...</td>\n",
       "      <td>This is an image of a street. The street is ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person is skiing through the snow. There is ...</td>\n",
       "      <td>A man is standing on top of a snowboard. He is...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is a bed in a room against a wall. There...</td>\n",
       "      <td>This is an image of a bed. The bed is made of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black and red train is on the tracks and has...</td>\n",
       "      <td>This is an image of a train station. The train...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A white and yellow public transportation bus w...</td>\n",
       "      <td>This is an image of a bus. The bus is white. T...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A large white house with a brown door sits beh...</td>\n",
       "      <td>This is an image of a sheep. The sheep is stan...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A man in a red and yellow t-shirt is holding a...</td>\n",
       "      <td>This is an image of a man and woman. The man i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There are three men riding bicycles. Two of th...</td>\n",
       "      <td>A man is riding a bike on a road. He is wearin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TWO CELL PHONES ON THE TABLE.THEY BOTH ARE DAR...</td>\n",
       "      <td>This is an image of a clock. The clock is blac...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A painted mannequin head sits on a table. It i...</td>\n",
       "      <td>This is an image of a clock. The clock is blac...</td>\n",
       "      <td>0</td>\n",
       "      <td>Model B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_caption  \\\n",
       "0  A large building with bars on the windows in f...   \n",
       "1  A person is skiing through the snow. There is ...   \n",
       "2  There is a bed in a room against a wall. There...   \n",
       "3  A black and red train is on the tracks and has...   \n",
       "4  A white and yellow public transportation bus w...   \n",
       "5  A large white house with a brown door sits beh...   \n",
       "6  A man in a red and yellow t-shirt is holding a...   \n",
       "7  There are three men riding bicycles. Two of th...   \n",
       "8  TWO CELL PHONES ON THE TABLE.THEY BOTH ARE DAR...   \n",
       "9  A painted mannequin head sits on a table. It i...   \n",
       "\n",
       "                                   generated_caption  occlusion_level    model  \n",
       "0  This is an image of a street. The street is ma...                0  Model B  \n",
       "1  A man is standing on top of a snowboard. He is...                0  Model B  \n",
       "2  This is an image of a bed. The bed is made of ...                0  Model B  \n",
       "3  This is an image of a train station. The train...                0  Model B  \n",
       "4  This is an image of a bus. The bus is white. T...                0  Model B  \n",
       "5  This is an image of a sheep. The sheep is stan...                0  Model B  \n",
       "6  This is an image of a man and woman. The man i...                0  Model B  \n",
       "7  A man is riding a bike on a road. He is wearin...                0  Model B  \n",
       "8  This is an image of a clock. The clock is blac...                0  Model B  \n",
       "9  This is an image of a clock. The clock is blac...                0  Model B  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/working/partc.csv\")\n",
    "df['model'] = df['model'].replace({'custom_model': 'Model B', 'smolvlm': 'Model A'})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:35:54.093631Z",
     "iopub.status.busy": "2025-04-10T10:35:54.092904Z",
     "iopub.status.idle": "2025-04-10T10:35:54.099326Z",
     "shell.execute_reply": "2025-04-10T10:35:54.098549Z",
     "shell.execute_reply.started": "2025-04-10T10:35:54.093605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CaptionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Based classifier for identifying which model generated the caption\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model='google-bert/bert-base-uncased', num_classes=2):\n",
    "        super(CaptionClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward passof the model\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input IDs for the BERT model.\n",
    "            attention_mask (torch.Tensor): Attention mask for the BERT model.\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits from the classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:35:57.361427Z",
     "iopub.status.busy": "2025-04-10T10:35:57.360857Z",
     "iopub.status.idle": "2025-04-10T10:35:57.366839Z",
     "shell.execute_reply": "2025-04-10T10:35:57.366119Z",
     "shell.execute_reply.started": "2025-04-10T10:35:57.361404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for caption classification task\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.label_map = {'Model A': 0, 'Model B': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        text = f\"{row['original_caption']} [SEP] {row['generated_caption']} [SEP] {row['occlusion_level']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text, add_special_tokens=True,\n",
    "            max_length=self.max_length, padding='max_length',\n",
    "            truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        label = self.label_map[row['model']]\n",
    "        \n",
    "        return{\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T10:36:00.477820Z",
     "iopub.status.busy": "2025-04-10T10:36:00.477255Z",
     "iopub.status.idle": "2025-04-10T10:36:00.486013Z",
     "shell.execute_reply": "2025-04-10T10:36:00.485292Z",
     "shell.execute_reply.started": "2025-04-10T10:36:00.477799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Training the BERT-based classifier\n",
    "    Args:\n",
    "        model (nn.Module): The caption classifier model.\n",
    "        train_dataloader (DataLoader): DataLoader for training data.\n",
    "        val_dataloader (DataLoader): DataLoader for validation data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (str): Device to train on (\"cuda\" or \"cpu\").\n",
    "        epochs (int, optional): Number of epochs to train. Defaults to 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_pbar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:11:18.918735Z",
     "iopub.status.busy": "2025-04-10T11:11:18.918180Z",
     "iopub.status.idle": "2025-04-10T11:11:18.925212Z",
     "shell.execute_reply": "2025-04-10T11:11:18.924456Z",
     "shell.execute_reply.started": "2025-04-10T11:11:18.918710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier\n",
    "    Args:\n",
    "        model (nn.Module): The caption classifier model.\n",
    "        test_dataloader (DataLoader): DataLoader for test data.\n",
    "        device (str): Device to evaluate on (\"cuda\" or \"cpu\").\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    test_pbar = tqdm(test_dataloader, desc=\"Evaluating Classifier\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Ensure data is moved to CPU and converted to numpy\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Sanity check: make sure lengths match and not empty\n",
    "    if len(all_preds) == 0 or len(all_labels) == 0:\n",
    "        raise ValueError(\"No predictions or labels found. Ensure your test_dataloader has data and model outputs correctly.\")\n",
    "\n",
    "    if len(all_preds) != len(all_labels):\n",
    "        raise ValueError(f\"Mismatch in predictions and labels: {len(all_preds)} preds vs {len(all_labels)} labels.\")\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluation Results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:11:23.071105Z",
     "iopub.status.busy": "2025-04-10T11:11:23.070433Z",
     "iopub.status.idle": "2025-04-10T11:11:23.077128Z",
     "shell.execute_reply": "2025-04-10T11:11:23.076386Z",
     "shell.execute_reply.started": "2025-04-10T11:11:23.071080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# since this part is small we can have a dedicated function to run the overall Part C\n",
    "def run_part_c():\n",
    "    # set seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"Using Device: {DEVICE}\")\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "    full_dataset = CaptionDataset(df, tokenizer=tokenizer)\n",
    "    \n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    \n",
    "    print(f\"Total Dataset Size: {dataset_size}\")\n",
    "    print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    batch_size = 16 # can be varied according to GPU constraints\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    model = CaptionClassifier().to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"Training Model...\")\n",
    "    model = train_classifier(model, train_dataloader, val_dataloader, optimizer, criterion, DEVICE, epochs=10)\n",
    "    print(\"Model Training Finished for the Classifier\")\n",
    "    \n",
    "    print(\"Evaluating Model...\")\n",
    "    results = evaluate_classifier(model, test_dataloader, DEVICE)\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:11:28.004851Z",
     "iopub.status.busy": "2025-04-10T11:11:28.004620Z",
     "iopub.status.idle": "2025-04-10T11:11:28.009343Z",
     "shell.execute_reply": "2025-04-10T11:11:28.008516Z",
     "shell.execute_reply.started": "2025-04-10T11:11:28.004835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    required_cols = ['original_caption', 'generated_caption', 'occlusion_level', 'model']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' is missing from your dataframe\")\n",
    "        \n",
    "    df['occlusion_level'] = df['occlusion_level'].astype(str)\n",
    "         \n",
    "    if not set(df['model']).issubset({'Model A', 'Model B'}):\n",
    "        raise ValueError(\"Model column should only contain 'Model A' and 'Model B' values\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Part C all at once using the main() funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:25:42.373244Z",
     "iopub.status.busy": "2025-04-10T11:25:42.372517Z",
     "iopub.status.idle": "2025-04-10T11:39:28.097551Z",
     "shell.execute_reply": "2025-04-10T11:39:28.096915Z",
     "shell.execute_reply.started": "2025-04-10T11:25:42.373221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Total Dataset Size: 7424\n",
      "Train size: 5196, Val size: 742, Test size: 1486\n",
      "Training Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.21it/s, loss=0.0531]\n",
      "Epoch 1/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.25it/s, loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.0914, Val Loss: 0.0392\n",
      "New best model saved with validation loss: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.21it/s, loss=0.0575]\n",
      "Epoch 2/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.11it/s, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.0318, Val Loss: 0.0414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.19it/s, loss=0.1372]\n",
      "Epoch 3/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 10.99it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.0283, Val Loss: 0.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.19it/s, loss=0.0005]\n",
      "Epoch 4/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 10.83it/s, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.0307, Val Loss: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.19it/s, loss=0.0004]\n",
      "Epoch 5/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.03it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.0283, Val Loss: 0.0382\n",
      "New best model saved with validation loss: 0.0382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.19it/s, loss=0.0002]\n",
      "Epoch 6/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.09it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.0254, Val Loss: 0.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.19it/s, loss=0.0002]\n",
      "Epoch 7/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.02it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.0249, Val Loss: 0.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.20it/s, loss=0.0001]\n",
      "Epoch 8/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.16it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.0252, Val Loss: 0.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.22it/s, loss=0.0001]\n",
      "Epoch 9/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.14it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.0240, Val Loss: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|██████████| 325/325 [01:17<00:00,  4.22it/s, loss=0.0001]\n",
      "Epoch 10/10 [Validation]: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.0267, Val Loss: 0.0748\n",
      "Model Training Finished for the Classifier\n",
      "Evaluating Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Classifier: 100%|██████████| 93/93 [00:08<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results - Precision: 0.9732, Recall: 0.9731, F1 Score: 0.9731\n",
      "\n",
      "Final Results:\n",
      "Precision: 0.9732\n",
      "Recall: 0.9731\n",
      "F1 Score: 0.9731\n",
      "Results and Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = prepare_dataframe(df)\n",
    "    \n",
    "    # run for Part C\n",
    "    model, results = run_part_c()\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "    \n",
    "    # save the results to csv\n",
    "    results_df = pd.DataFrame({\n",
    "        'metric': ['precision', 'recall', 'f1'],\n",
    "        'value': [results['precision'], results['recall'], results['f1_score']]\n",
    "    })\n",
    "    results_df.to_csv('part_c_results.csv', index=False)\n",
    "    torch.save(model.state_dict(), \"caption_classifier_model.pth\")\n",
    "    print(\"Results and Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6994720,
     "sourceId": 11202873,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
