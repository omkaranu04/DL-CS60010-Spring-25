{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate rouge_score\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    ViTModel,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    ViTFeatureExtractor\n",
    ")\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /kaggle/working/nltk_data/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_dir = \"/kaggle/working/nltk_data\"\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/\n",
    "!unzip -o /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_captioning(image_path, model_name=\"HuggingFaceTB/SmolVLM-256M-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generate captions using the pre-trained SmolVLM model without training.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model_name (str): Name of the pre-trained model (default: SmolVLM).\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated caption\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(zero_shot_captioning, \"model\") or not hasattr(zero_shot_captioning, \"processor\"):\n",
    "        print(\"Loading model and processor...\")\n",
    "        zero_shot_captioning.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        zero_shot_captioning.model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            device_map=DEVICE,\n",
    "            _attn_implementation=\"eager\"\n",
    "        ).to(DEVICE)\n",
    "        print(\"Model and processor loaded successfully!\")\n",
    "        \n",
    "    try:\n",
    "        image = load_image(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Loading image from {image_path} failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = zero_shot_captioning.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = zero_shot_captioning.processor(text=prompt, images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = zero_shot_captioning.model.generate(**inputs, max_new_tokens=100)\n",
    "        generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "        generated_text = zero_shot_captioning.processor.batch_decode(\n",
    "            generated_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_captions(test_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Generate captions for all images in a directory and save them to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        test_dir (str): Path to the directory containing test images\n",
    "        output_csv_path (str): Path to save the output CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Generating captions\"):\n",
    "        image_path = os.path.join(test_dir, image_file)\n",
    "        caption = zero_shot_captioning(image_path)\n",
    "        \n",
    "        if caption:\n",
    "            results.append({\n",
    "                'filename': image_file,\n",
    "                'generated_caption': caption\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv_path, index=True)\n",
    "    print(f\"Captions saved to {output_csv_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_csv_path, generated_csv_path):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using BLEU, ROUGE-L, METEOR.\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path (str): Path to the CSV with ground truth captions\n",
    "        generated_csv_path (str): Path to the CSV with generated captions\n",
    "        \n",
    "    Returns:\n",
    "        dict: BLEU, ROUGE-L, METEOR scores for the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    generated_df = pd.read_csv(generated_csv_path)\n",
    "    \n",
    "    merged_df = pd.merge(test_df, generated_df, on='filename', how='inner')\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        print(\"No matching filenames found between test and generated captions.\")\n",
    "        return None\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for _, row in merged_df.iterrows():\n",
    "        reference = nltk.word_tokenize(row['caption'].lower())\n",
    "        hypothesis = nltk.word_tokenize(row['generated_caption'].lower())\n",
    "        \n",
    "        references.append([reference])\n",
    "        hypotheses.append(hypothesis)\n",
    "        \n",
    "    # BLEU score\n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "    \n",
    "    # ROUGE-L score\n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for i in range(len(merged_df)):\n",
    "        score = rouge.score(merged_df.iloc[i]['caption'], merged_df.iloc[i]['generated_caption'])\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)\n",
    "    rouge_l_score = np.mean(rouge_scores)\n",
    "    \n",
    "    # METEOR score\n",
    "    meteor_scores = []\n",
    "    for i in range(len(merged_df)):\n",
    "        ref = nltk.word_tokenize(merged_df.iloc[i]['caption'].lower())\n",
    "        hyp = nltk.word_tokenize(merged_df.iloc[i]['generated_caption'].lower())\n",
    "        score = meteor_score([ref], hyp)\n",
    "        meteor_scores.append(score)\n",
    "    meteor_score_avg = np.mean(meteor_scores)\n",
    "    \n",
    "    results = {\n",
    "        'BLEU': bleu_score,\n",
    "        'ROUGE-L': rouge_l_score,\n",
    "        'METEOR': meteor_score_avg\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/kaggle/input/dataset/Dataset/test\"\n",
    "generated_csv_path = \"smolvlm_captions.csv\"\n",
    "test_csv_path = \"/kaggle/input/dataset/Dataset/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating captions for test images...\")\n",
    "generate_and_save_captions(test_dir, generated_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model performance...\")\n",
    "evaluation_results = evaluate_model(test_csv_path, generated_csv_path)\n",
    "if evaluation_results:\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"BLEU Score: {evaluation_results['BLEU']:.4f}\")\n",
    "    print(f\"ROUGE-L Score: {evaluation_results['ROUGE-L']:.4f}\")\n",
    "    print(f\"METEOR Score: {evaluation_results['METEOR']:.4f}\")\n",
    "df_results = pd.DataFrame([evaluation_results]) \n",
    "df_results.to_csv(\"smolvlm_results.csv\", index=False)\n",
    "print(\"Evaluation results saved as smolvlm_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for image captioning with ViT and GPT2\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dir, tokenizer, max_length=50, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx]['filename'])\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "            \n",
    "        # tokenize captions\n",
    "        caption_encoding = self.tokenizer(\n",
    "            caption, padding='max_length',\n",
    "            truncation=True, max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        caption_ids = caption_encoding.input_ids.squeeze(0)\n",
    "        \n",
    "        return image, caption_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Image Captioning Model combining ViT and GPT2\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model='WinKawaks/vit-small-patch16-224', gpt2_model='gpt2', freeze_vit=True, freeze_gpt2_partial=True):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        \n",
    "        self.encoder = ViTModel.from_pretrained(vit_model)\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        if freeze_vit:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model)\n",
    "        self.decoder_dim = self.decoder.config.hidden_size\n",
    "        \n",
    "        if freeze_gpt2_partial:\n",
    "            for i, block in enumerate(self.decoder.transformer.h):\n",
    "                if i < len(self.decoder.transformer.h) - 2:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "        self.connect = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.img_token_id = self.tokenizer.convert_tokens_to_ids(\"<|img|>\") if \"<|img|>\" in self.tokenizer.get_vocab() else self.tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "        \n",
    "    def forward(self, images, captions=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            images: Tensor of input images [batch_size, channels, height, width]\n",
    "            captions: Optional tensor of tokenized captions for training\n",
    "            \n",
    "        Returns:\n",
    "            Tensor containing model predictions (logits)\n",
    "        \"\"\"\n",
    "        encoder_outputs = self.encoder(images).last_hidden_state\n",
    "        cls_output = encoder_outputs[:, 0, :]\n",
    "        img_features = self.connect(cls_output)\n",
    "        \n",
    "        if captions is not None:\n",
    "            outputs = self.decoder(input_ids=captions, labels=captions, encoder_hidden_states=img_features.unsqueeze(1))\n",
    "            return outputs.logits\n",
    "        else:\n",
    "            batch_size = images.size(0)\n",
    "            input_ids = torch.ones((batch_size, 1), dtype=torch.long, device=images.device) * self.img_token_id\n",
    "            \n",
    "            encoder_outputs = img_features.unsqueeze(1)\n",
    "            \n",
    "            outputs = self.decoder.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                encoder_hidden_states=encoder_outputs,\n",
    "            )\n",
    "            \n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_csv, val_csv, train_img_dir, val_img_dir, tokenizer, batch_szie=8):\n",
    "    train_dataset = ImageCaptionDataset(csv_file=train_csv, img_dir=train_img_dir, tokenizer=tokenizer)\n",
    "    val_dataset = ImageCaptionDataset(csv_file=val_csv, img_dir=val_img_dir, tokenizer=tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_szie, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_szie, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3, save_path=\"best_custom_model.pth\"):\n",
    "    \"\"\"\n",
    "    Train the encoder-decoder model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Custom image captioning model.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        optimizer: Optimizer (e.g., Adam).\n",
    "        criterion (Loss): Loss function.\n",
    "        device (str): Device to use ('cuda' or 'cpu').\n",
    "        epochs (int): Number of epochs.\n",
    "        save_path (str): Path to save the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n",
    "        \n",
    "        for images, captions in train_pbar:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            outputs = model(images, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss' : loss.item()})\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, captions in val_pbar:\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                outputs = model(images, captions)\n",
    "                \n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), captions.view(-1))\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({'loss' : loss.item()})\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_with_custom_model(model, image_path, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Generate caption for a single image using the custom model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained custom model\n",
    "        image_path: Path to the image\n",
    "        tokenizer: GPT2 tokenizer\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model(image)\n",
    "        \n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_for_test_set(model, test_dir, output_csv, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Generate captions for all images in test directory using custom model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained custom model\n",
    "        test_dir: Directory containing test images\n",
    "        output_csv: Path to save output CSV\n",
    "        tokenizer: GPT2 tokenizer\n",
    "        device: Device to use\n",
    "    \"\"\"\n",
    "    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]\n",
    "    image_files.sort()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Generating captions\"):\n",
    "        image_path = os.path.join(test_dir, image_file)\n",
    "        caption = generate_captions_with_custom_model(model, image_path, tokenizer, device)\n",
    "        \n",
    "        if caption:\n",
    "            results.append({\n",
    "                'filename': image_file,\n",
    "                'generated_caption': caption\n",
    "            })\n",
    "            \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Captions saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    train_csv = \"train.csv\"\n",
    "    val_csv = \"val.csv\"\n",
    "    test_csv = \"test.csv\"\n",
    "    train_img_dir = \"train\"\n",
    "    val_img_dir = \"val\"\n",
    "    test_img_dir = \"test\"\n",
    "    \n",
    "    # Initialize model, tokenizer, criterion, optimizer\n",
    "    model = ImageCaptionModel()\n",
    "    tokenizer = model.tokenizer\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        train_csv=train_csv,\n",
    "        val_csv=val_csv,\n",
    "        train_img_dir=train_img_dir,\n",
    "        val_img_dir=val_img_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=16  # Adjust based on your GPU memory\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer and criterion\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        epochs=10,  # Adjust as needed\n",
    "        save_path=\"best_custom_model.pth\"\n",
    "    )\n",
    "    \n",
    "    # Load the best model for inference\n",
    "    print(\"Loading best model for inference...\")\n",
    "    model.load_state_dict(torch.load(\"best_custom_model.pth\"))\n",
    "    \n",
    "    # Generate captions for test set\n",
    "    print(\"Generating captions for test set...\")\n",
    "    generate_captions_for_test_set(\n",
    "        model=model,\n",
    "        test_dir=test_img_dir,\n",
    "        output_csv=\"custom_model_captions.csv\",\n",
    "        tokenizer=tokenizer,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model performance...\")\n",
    "    evaluation_results = evaluate_model(\n",
    "        test_csv_path=test_csv,\n",
    "        generated_csv_path=\"custom_model_captions.csv\"\n",
    "    )\n",
    "    \n",
    "    if evaluation_results:\n",
    "        print(\"\\nEvaluation Results for Custom Model:\")\n",
    "        print(f\"BLEU Score: {evaluation_results['BLEU']:.4f}\")\n",
    "        print(f\"ROUGE-L Score: {evaluation_results['ROUGE-L']:.4f}\")\n",
    "        print(f\"METEOR Score: {evaluation_results['METEOR']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlude_image(image, mask_percentage):\n",
    "    \"\"\"\n",
    "    Apply patch-wise occlusion to an image.\n",
    "    \n",
    "    Args:\n",
    "        image (np.array): Input image.\n",
    "        mask_percentage (int): Percentage of image to be masked.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Occluded image.\n",
    "    \"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    \n",
    "    occluded_image = image.copy()\n",
    "    \n",
    "    if len(image.shape) == 3: \n",
    "        height, width, channels = image.shape\n",
    "    else: \n",
    "        height, width = image.shape\n",
    "        channels = 1\n",
    "        occluded_image = occluded_image.reshape(height, width, channels)\n",
    "    \n",
    "    patch_height = height // 16\n",
    "    patch_width = width // 16\n",
    "    \n",
    "    total_patches = 16 * 16\n",
    "    patches_to_mask = int((mask_percentage / 100) * total_patches)\n",
    "    \n",
    "    patch_indices = np.arange(total_patches)\n",
    "    np.random.shuffle(patch_indices)\n",
    "    masked_patches = patch_indices[:patches_to_mask]\n",
    "    \n",
    "    for patch_idx in masked_patches:\n",
    "        row = patch_idx // 16\n",
    "        col = patch_idx % 16\n",
    "        \n",
    "        row_start = row * patch_height\n",
    "        row_end = min((row + 1) * patch_height, height)\n",
    "        col_start = col * patch_width\n",
    "        col_end = min((col + 1) * patch_width, width)\n",
    "        \n",
    "        occluded_image[row_start:row_end, col_start:col_end, :] = 0\n",
    "    \n",
    "    if channels == 1 and len(image.shape) == 2:\n",
    "        occluded_image = occluded_image.reshape(height, width)\n",
    "    \n",
    "    return occluded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccludedImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for occluded images with their captions\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dir, occlusion_percentage=0, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to CSV with image filenames and captions\n",
    "            img_dir: Directory containing images\n",
    "            occlusion_percentage: Percentage of image to occlude\n",
    "            transform: Optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.occlusion_percentage = occlusion_percentage\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx]['filename'])\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        filename = self.df.iloc[idx]['filename']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            if self.occlusion_percentage > 0:\n",
    "                occluded_image_np = occlude_image(image_np, self.occlusion_percentage)\n",
    "                image = Image.fromarray(occluded_image_np.astype(np.uint8))\n",
    "            \n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        return image, caption, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_occluded_images(model, dataloader, device, model_name, occlusion_level, metrics_only=False):\n",
    "    \"\"\"\n",
    "    Evaluate performance after occluding images.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Image captioning model.\n",
    "        dataloader (DataLoader): Test dataloader.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "        model_name (str): Name of the model ('SmolVLM' or 'Custom')\n",
    "        occlusion_level (int): Current occlusion percentage.\n",
    "        metrics_only (bool): If True, only return metrics without saving CSVs\n",
    "    \n",
    "    Returns:\n",
    "        dict: BLEU, ROUGE-L, METEOR scores for the test set.\n",
    "        DataFrame: Generated captions with filenames\n",
    "    \"\"\"\n",
    "    is_custom_model = not isinstance(model, (AutoModelForVision2Seq))\n",
    "    \n",
    "    generated_captions = []\n",
    "    reference_captions = []\n",
    "    image_filenames = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, captions, filenames in tqdm(dataloader, desc=f\"Evaluating {model_name} at {occlusion_level}% occlusion\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            if is_custom_model:\n",
    "                output_ids = model(images)\n",
    "                generated_caption = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\"},\n",
    "                            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "                prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "                inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "                generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "                generated_caption = processor.batch_decode(\n",
    "                    generated_ids,\n",
    "                    skip_special_tokens=True,\n",
    "                )[0].strip()\n",
    "            \n",
    "            for i in range(len(captions)):\n",
    "                generated_captions.append(generated_caption)\n",
    "                reference_captions.append(captions[i])\n",
    "                image_filenames.append(filenames[i])\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'filename': image_filenames,\n",
    "        'original_caption': reference_captions,\n",
    "        'generated_caption': generated_captions,\n",
    "        'occlusion_level': occlusion_level,\n",
    "        'model': model_name\n",
    "    })\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for i in range(len(results_df)):\n",
    "        reference = nltk.word_tokenize(results_df.iloc[i]['original_caption'].lower())\n",
    "        hypothesis = nltk.word_tokenize(results_df.iloc[i]['generated_caption'].lower())\n",
    "        \n",
    "        references.append([reference])  \n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "    \n",
    "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for i in range(len(results_df)):\n",
    "        score = rouge.score(results_df.iloc[i]['original_caption'], results_df.iloc[i]['generated_caption'])\n",
    "        rouge_scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    rouge_l_score = np.mean(rouge_scores)\n",
    "    \n",
    "    meteor_scores = []\n",
    "    for i in range(len(results_df)):\n",
    "        ref = nltk.word_tokenize(results_df.iloc[i]['original_caption'].lower())\n",
    "        hyp = nltk.word_tokenize(results_df.iloc[i]['generated_caption'].lower())\n",
    "        score = meteor_score([ref], hyp)\n",
    "        meteor_scores.append(score)\n",
    "    \n",
    "    meteor_score_avg = np.mean(meteor_scores)\n",
    "    \n",
    "    metrics = {\n",
    "        'BLEU': bleu_score,\n",
    "        'ROUGE-L': rouge_l_score,\n",
    "        'METEOR': meteor_score_avg\n",
    "    }\n",
    "    \n",
    "    if not metrics_only:\n",
    "        output_csv = f\"{model_name.lower()}_occlusion_{occlusion_level}.csv\"\n",
    "        results_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Results saved to {output_csv}\")\n",
    "        \n",
    "        part_c_data = []\n",
    "        for i in range(len(results_df)):\n",
    "            # Format as required for Part C\n",
    "            input_text = f\"{results_df.iloc[i]['original_caption']} <SEP> {results_df.iloc[i]['generated_caption']} <SEP> {occlusion_level}\"\n",
    "            output_label = \"Model A\" if model_name == \"SmolVLM\" else \"Model B\"\n",
    "            \n",
    "            part_c_data.append({\n",
    "                'input_text': input_text,\n",
    "                'output_label': output_label\n",
    "            })\n",
    "        \n",
    "        part_c_df = pd.DataFrame(part_c_data)\n",
    "        \n",
    "        if os.path.exists('partc.csv'):\n",
    "            part_c_df.to_csv('partc.csv', mode='a', header=False, index=False)\n",
    "        else:\n",
    "            part_c_df.to_csv('partc.csv', index=False)\n",
    "    \n",
    "    return metrics, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_occlusion_study(test_csv, test_img_dir, custom_model_path, occlusion_levels=[10, 50, 80]):\n",
    "    \"\"\"\n",
    "    Run a complete occlusion study for both models at different levels.\n",
    "    \n",
    "    Args:\n",
    "        test_csv: Path to test CSV file\n",
    "        test_img_dir: Directory containing test images\n",
    "        custom_model_path: Path to saved custom model\n",
    "        occlusion_levels: List of occlusion percentages to test\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for all models and occlusion levels\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    all_results = {\n",
    "        'SmolVLM': {},\n",
    "        'Custom': {}\n",
    "    }\n",
    "    \n",
    "    # Load SmolVLM model\n",
    "    print(\"Loading SmolVLM model...\")\n",
    "    processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "    smolVLM_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        _attn_implementation=\"eager\"\n",
    "    ).to(device)\n",
    "    \n",
    "    print(\"Loading custom model...\")\n",
    "    custom_model = ImageCaptionModel()\n",
    "    custom_model.load_state_dict(torch.load(custom_model_path, map_location=device))\n",
    "    custom_model = custom_model.to(device)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nEvaluating on original images (0% occlusion)...\")\n",
    "    \n",
    "    original_dataset = OccludedImageDataset(\n",
    "        csv_file=test_csv,\n",
    "        img_dir=test_img_dir,\n",
    "        occlusion_percentage=0,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    original_loader = DataLoader(\n",
    "        original_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    baseline_smolVLM, _ = evaluate_on_occluded_images(\n",
    "        model=smolVLM_model, \n",
    "        dataloader=original_loader,\n",
    "        device=device,\n",
    "        model_name=\"SmolVLM\",\n",
    "        occlusion_level=0\n",
    "    )\n",
    "    \n",
    "    all_results['SmolVLM'][0] = baseline_smolVLM\n",
    "    \n",
    "    baseline_custom, _ = evaluate_on_occluded_images(\n",
    "        model=custom_model, \n",
    "        dataloader=original_loader,\n",
    "        device=device,\n",
    "        model_name=\"Custom\",\n",
    "        occlusion_level=0\n",
    "    )\n",
    "    \n",
    "    all_results['Custom'][0] = baseline_custom\n",
    "    \n",
    "    for occlusion in occlusion_levels:\n",
    "        print(f\"\\nEvaluating at {occlusion}% occlusion...\")\n",
    "        \n",
    "        occluded_dataset = OccludedImageDataset(\n",
    "            csv_file=test_csv,\n",
    "            img_dir=test_img_dir,\n",
    "            occlusion_percentage=occlusion,\n",
    "            transform=transform\n",
    "        )\n",
    "        \n",
    "        occluded_loader = DataLoader(\n",
    "            occluded_dataset,\n",
    "            batch_size=8,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        smolVLM_metrics, _ = evaluate_on_occluded_images(\n",
    "            model=smolVLM_model, \n",
    "            dataloader=occluded_loader,\n",
    "            device=device,\n",
    "            model_name=\"SmolVLM\",\n",
    "            occlusion_level=occlusion\n",
    "        )\n",
    "        \n",
    "        all_results['SmolVLM'][occlusion] = smolVLM_metrics\n",
    "        \n",
    "        custom_metrics, _ = evaluate_on_occluded_images(\n",
    "            model=custom_model, \n",
    "            dataloader=occluded_loader,\n",
    "            device=device,\n",
    "            model_name=\"Custom\",\n",
    "            occlusion_level=occlusion\n",
    "        )\n",
    "        \n",
    "        all_results['Custom'][occlusion] = custom_metrics\n",
    "    \n",
    "    print(\"\\nCalculating metric changes across occlusion levels...\")\n",
    "    metric_changes = {\n",
    "        'SmolVLM': {occlusion: {} for occlusion in occlusion_levels},\n",
    "        'Custom': {occlusion: {} for occlusion in occlusion_levels}\n",
    "    }\n",
    "    \n",
    "    for model_name in ['SmolVLM', 'Custom']:\n",
    "        for occlusion in occlusion_levels:\n",
    "            for metric in ['BLEU', 'ROUGE-L', 'METEOR']:\n",
    "                # Calculate change from baseline (0% occlusion)\n",
    "                change = all_results[model_name][occlusion][metric] - all_results[model_name][0][metric]\n",
    "                metric_changes[model_name][occlusion][metric] = change\n",
    "                \n",
    "                print(f\"{model_name} {metric} change at {occlusion}% occlusion: {change:.4f}\")\n",
    "    \n",
    "    return all_results, metric_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = \"/kaggle/input/dataset/Dataset/test.csv\"\n",
    "test_img_dir = \"/kaggle/input/dataset/Dataset/test\"\n",
    "custom_model_path = \"best_custom_model.pth\"\n",
    "occlusion_levels = [10]\n",
    "\n",
    "print(\"Starting Occlusion Images ...\")\n",
    "all_results, metric_changes = run_occlusion_study(\n",
    "    test_csv=test_csv,\n",
    "    test_img_dir=test_img_dir,\n",
    "    custom_model_path=custom_model_path,\n",
    "    occlusion_levels=occlusion_levels\n",
    ")\n",
    "print(\"\\nOcclusion study completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics at different Occlusion Levels : \")\n",
    "for model_name in ['SmolVLM', 'Custom']:\n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    print(f\"{'Occlusion Level':<15} {'BLEU':<10} {'ROUGE-L':<10} {'METEOR':<10}\")\n",
    "    \n",
    "    bleu = all_results[model_name][0]['BLEU']\n",
    "    rouge = all_results[model_name][0]['ROUGE-L']\n",
    "    meteor = all_results[model_name][0]['METEOR']\n",
    "    \n",
    "    print(f\"{'0% (baseline)':<15} {bleu:.4f}      {rouge:.4f}      {meteor:.4f}\")\n",
    "    \n",
    "    for occlusion in occlusion_levels:\n",
    "        bleu = all_results[model_name][occlusion]['BLEU']\n",
    "        rouge = all_results[model_name][occlusion]['ROUGE-L']\n",
    "        meteor = all_results[model_name][occlusion]['METEOR']\n",
    "        \n",
    "        bleu_change = metric_changes[model_name][occlusion]['BLEU']\n",
    "        rouge_change = metric_changes[model_name][occlusion]['ROUGE-L']\n",
    "        meteor_change = metric_changes[model_name][occlusion]['METEOR']\n",
    "        \n",
    "        print(f\"{f'{occlusion}%':<15} {bleu:.4f}      {rouge:.4f}      {meteor:.4f}\")\n",
    "        print(f\"{'Change':<15} {bleu_change:+.4f}      {rouge_change:+.4f}      {meteor_change:+.4f}\")\n",
    "\n",
    "print(\"dataset saved ot partc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based classifier for identifying which model generated a caption.\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
    "        super(CaptionClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # BERT hidden size\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Add dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs\n",
    "            attention_mask: Attention mask for BERT\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits for classification\n",
    "        \"\"\"\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use the [CLS] token representation for classification\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionClassifierDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the caption classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame containing input_text and output_label columns\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Map text labels to numeric labels\n",
    "        self.label_map = {\n",
    "            'Model A': 0,  # SmolVLM\n",
    "            'Model B': 1   # Custom\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Get input text and label\n",
    "        input_text = self.dataframe.iloc[idx]['input_text']\n",
    "        label = self.dataframe.iloc[idx]['output_label']\n",
    "        \n",
    "        # Convert label to numeric\n",
    "        numeric_label = self.label_map[label]\n",
    "        \n",
    "        # Tokenize input text\n",
    "        encoding = self.tokenizer(\n",
    "            text=input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(numeric_label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_part_c_dataset(part_c_csv, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Split the Part C dataset into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        part_c_csv: Path to the Part C CSV file\n",
    "        train_ratio: Ratio of data for training\n",
    "        val_ratio: Ratio of data for validation\n",
    "        test_ratio: Ratio of data for testing\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(part_c_csv)\n",
    "    \n",
    "    # Extract unique image identifiers from filenames in the input text\n",
    "    df['image_id'] = df['input_text'].apply(lambda x: x.split('<SEP>')[0].strip())\n",
    "    \n",
    "    # Get unique image IDs\n",
    "    unique_images = df['image_id'].unique()\n",
    "    np.random.shuffle(unique_images)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    n_train = int(len(unique_images) * train_ratio)\n",
    "    n_val = int(len(unique_images) * val_ratio)\n",
    "    \n",
    "    # Split image IDs\n",
    "    train_images = unique_images[:n_train]\n",
    "    val_images = unique_images[n_train:n_train+n_val]\n",
    "    test_images = unique_images[n_train+n_val:]\n",
    "    \n",
    "    # Create masks for each split\n",
    "    train_mask = df['image_id'].isin(train_images)\n",
    "    val_mask = df['image_id'].isin(val_images)\n",
    "    test_mask = df['image_id'].isin(test_images)\n",
    "    \n",
    "    # Create DataFrames for each split\n",
    "    train_df = df[train_mask].reset_index(drop=True)\n",
    "    val_df = df[val_mask].reset_index(drop=True)\n",
    "    test_df = df[test_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Training samples: {len(train_df)} ({len(train_df)/len(df):.2%})\")\n",
    "    print(f\"Validation samples: {len(val_df)} ({len(val_df)/len(df):.2%})\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(df):.2%})\")\n",
    "    \n",
    "    # Remove the temporary image_id column\n",
    "    train_df = train_df.drop(columns=['image_id'])\n",
    "    val_df = val_df.drop(columns=['image_id'])\n",
    "    test_df = test_df.drop(columns=['image_id'])\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, criterion, device, epochs=5):\n",
    "    \"\"\"\n",
    "    Train the BERT-based caption classifier.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Caption classifier model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        optimizer: Optimizer (e.g., Adam)\n",
    "        criterion (Loss): Loss function\n",
    "        device (str): Device to use ('cuda' or 'cpu')\n",
    "        epochs (int): Number of epochs\n",
    "    \"\"\"\n",
    "    # Best model tracking\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in train_pbar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': loss.item(), 'accuracy': train_correct/train_total})\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': loss.item(), 'accuracy': val_correct/val_total})\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save best model to disk\n",
    "    torch.save(best_model_state, 'best_caption_classifier.pth')\n",
    "    print(f\"Best model saved to 'best_caption_classifier.pth'\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the classification model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        dataloader (DataLoader): Test data loader\n",
    "        device (str): 'cuda' or 'cpu'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Precision, Recall and F1 scores for the test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Evaluate model\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "    \n",
    "    # Get precision, recall, f1 for each class (macro averaging)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_preds, \n",
    "        target_names=['Model A (SmolVLM)', 'Model B (Custom)']\n",
    "    ))\n",
    "    \n",
    "    # Return metrics\n",
    "    metrics = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to implement the function calls for PartC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
