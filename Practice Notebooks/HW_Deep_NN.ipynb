{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **CS60010 - Deep Learning 2025**\n",
        "HW Assignment - Deep Neural Networks\n",
        "\n",
        "Credits: Understanding Deep Learning by Simon J.D. Prince"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaKn8CFlzN8E"
      },
      "source": [
        "# 4.1 -- Composing networks\n",
        "\n",
        "The purpose of this section is to understand what happens when we feed one neural network into another. It works through an example similar to 4.1 and varies both networks\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ClURpZQzI6L"
      },
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdmveeAUz4YG"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ximCLwIfz8kj"
      },
      "outputs": [],
      "source": [
        "# Define a shallow neural network with, one input, one output, and three hidden units\n",
        "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
        "  # Initial lines\n",
        "  pre_1 = theta_10 + theta_11 * x\n",
        "  pre_2 = theta_20 + theta_21 * x\n",
        "  pre_3 = theta_30 + theta_31 * x\n",
        "  # Activation functions\n",
        "  act_1 = activation_fn(pre_1)\n",
        "  act_2 = activation_fn(pre_2)\n",
        "  act_3 = activation_fn(pre_3)\n",
        "  # Weight activations\n",
        "  w_act_1 = phi_1 * act_1\n",
        "  w_act_2 = phi_2 * act_2\n",
        "  w_act_3 = phi_3 * act_3\n",
        "  # Combine weighted activation and add y offset\n",
        "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
        "  # Return everything we have calculated\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB2HTalOE40X"
      },
      "outputs": [],
      "source": [
        "# # Plot two shallow neural networks and the composition of the two\n",
        "def plot_neural_two_components(x_in, net1_out, net2_out, net12_out=None):\n",
        "\n",
        "  # Plot the two networks separately\n",
        "  fig, ax = plt.subplots(1,2)\n",
        "  fig.set_size_inches(8.5, 8.5)\n",
        "  fig.tight_layout(pad=3.0)\n",
        "  ax[0].plot(x_in, net1_out,'r-')\n",
        "  ax[0].set_xlabel('Net 1 input'); ax[0].set_ylabel('Net 1 output')\n",
        "  ax[0].set_xlim([-1,1]);ax[0].set_ylim([-1,1])\n",
        "  ax[0].set_aspect(1.0)\n",
        "  ax[1].plot(x_in, net2_out,'b-')\n",
        "  ax[1].set_xlabel('Net 2 input'); ax[1].set_ylabel('Net 2 output')\n",
        "  ax[1].set_xlim([-1,1]);ax[1].set_ylim([-1,1])\n",
        "  ax[1].set_aspect(1.0)\n",
        "  plt.show()\n",
        "\n",
        "  if net12_out is not None:\n",
        "    # Plot their composition\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x_in ,net12_out,'g-')\n",
        "    ax.set_xlabel('Net 1 Input'); ax.set_ylabel('Net 2 Output')\n",
        "    ax.set_xlim([-1,1]);ax.set_ylim([-1,1])\n",
        "    ax.set_aspect(1.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxBJCObC-NTY"
      },
      "source": [
        "Let's define two networks.  We'll put the prefixes n1_ and n2_ before all the variables to make it clear which network is which.  We'll just consider the inputs and outputs over the range [-1,1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRebvurv22pT"
      },
      "outputs": [],
      "source": [
        "# Now lets define some parameters and run the first neural network\n",
        "n1_theta_10 = 0.0   ; n1_theta_11 = -1.0\n",
        "n1_theta_20 = 0     ; n1_theta_21 = 1.0\n",
        "n1_theta_30 = -0.67 ; n1_theta_31 =  1.0\n",
        "n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3\n",
        "\n",
        "# Now lets define some parameters and run the second neural network\n",
        "n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0\n",
        "n2_theta_20 =  0.2  ; n2_theta_21 = 1.0\n",
        "n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0\n",
        "n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0\n",
        "\n",
        "# Display the two inputs\n",
        "x = np.arange(-1,1,0.001)\n",
        "# We run the first  and second neural networks for each of these input values\n",
        "net1_out = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "net2_out = shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "# Plot both graphs\n",
        "plot_neural_two_components(x, net1_out, net2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUQVop9-Xta1"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Take a piece of paper and draw what you think will happen when we feed the\n",
        "# output of the first network into the second one.  Draw the relationship between\n",
        "# the input of the first network and the output of the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq7GH-MCIyPI"
      },
      "outputs": [],
      "source": [
        "# Now let's see if your predictions were right\n",
        "\n",
        "# TODO feed the output of first network into second network (replace this line)\n",
        "net12_out = np.zeros_like(x)\n",
        "\n",
        "# Plot all three graphs\n",
        "plot_neural_two_components(x, net1_out, net2_out, net12_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMlLkLbdEuPu"
      },
      "outputs": [],
      "source": [
        "# Now we'll change things a up a bit.  What happens if we change the second network? (note the *-1 change)\n",
        "net1_out = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "net2_out = shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of6jVXLTJ688"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Take a piece of paper and draw what you think will happen when we feed the\n",
        "# output of the first network into the modified second network.  Draw the relationship between\n",
        "# the input of the first network and the output of the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbbSCaSeK6SM"
      },
      "outputs": [],
      "source": [
        "# When you have a prediction, run this code to see if you were right\n",
        "net12_out = shallow_1_1_3(net1_out, ReLU, n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out, net12_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b39mcSGFK9Fd"
      },
      "outputs": [],
      "source": [
        "# Let's change things again.  What happens if we change the first network? (note the changes)\n",
        "net1_out = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1*0.5, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "net2_out = shallow_1_1_3(x, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhO40cC_LW9I"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Take a piece of paper and draw what you think will happen when we feed the\n",
        "# output of the modified first network into the original second network.  Draw the relationship between\n",
        "# the input of the first network and the output of the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akwo-hnPLkNr"
      },
      "outputs": [],
      "source": [
        "# When you have a prediction, run this code to see if you were right\n",
        "net12_out = shallow_1_1_3(net1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out, net12_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ7wXKpRLl_E"
      },
      "outputs": [],
      "source": [
        "# Let's change things again.  What happens if the first network and second networks are the same?\n",
        "net1_out = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "net2_out_new = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJbbh6R7NG9k"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Take a piece of paper and draw what you think will happen when we feed the\n",
        "# output of the first network into the a copy of itself.  Draw the relationship between\n",
        "# the input of the first network and the output of the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiZZl3yNM2Bq"
      },
      "outputs": [],
      "source": [
        "# When you have a prediction, run this code to see if you were right\n",
        "net12_out = shallow_1_1_3(net1_out, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "plot_neural_two_components(x, net1_out, net2_out_new, net12_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSd51AkzNf7-"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Contemplate what you think will happen when we feed the\n",
        "# output of the original first network into a second copy of the original first network, and then\n",
        "# the output of that into the original second network (so now we have a three layer network)\n",
        "# How many total linear regions will we have in the output?\n",
        "net123_out = shallow_1_1_3(net12_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "plot_neural_two_components(x, net12_out, net2_out, net123_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqzePCLOVQK7"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "# How many linear regions would there be if we ran N copies of the first network, feeding the result of the first\n",
        "# into the second, the second into the third and so on, and then passed the result into the original second\n",
        "# network (blue curve above)\n",
        "\n",
        "# Take away conclusion:  with very few parameters, we can make A LOT of linear regions, but\n",
        "# they depend on one another in complex ways that quickly become too difficult to understand intuitively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.2 -- Clipping functions\n",
        "\n",
        "Here we understand how a neural network with two hidden layers build more complicated functions by clipping and recombining the representations at the intermediate hidden variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a deep neural network with, one input, one output, two hidden layers and three hidden units (eqns 4.7-4.9, given on page 21 in CS60010_Deep_NN.pptx (1) 2.pdf)\n",
        "# To make this easier, we store the parameters in ndarrays, so phi_0 = phi[0] and psi_3,3 = psi[3,3] etc.\n",
        "def shallow_1_1_3_3(x, activation_fn, phi, psi, theta):\n",
        "\n",
        "  # TODO -- You write this function\n",
        "  # Replace the skeleton code below.\n",
        "\n",
        "  # ANSWER\n",
        "  # Preactivations at layer 1 (terms in brackets in equation 4.7)\n",
        "  layer1_pre_1 = np.zeros_like(x) ;\n",
        "  layer1_pre_2 = np.zeros_like(x) ;\n",
        "  layer1_pre_3 = np.zeros_like(x) ;\n",
        "\n",
        "  # Activation functions (rest of equation 4.7)\n",
        "  h1 = activation_fn(layer1_pre_1)\n",
        "  h2 = activation_fn(layer1_pre_2)\n",
        "  h3 = activation_fn(layer1_pre_3)\n",
        "\n",
        "  # Preactivations at layer 2 (terms in brackets in equation 4.8)\n",
        "  layer2_pre_1 = np.zeros_like(x) ;\n",
        "  layer2_pre_2 = np.zeros_like(x) ;\n",
        "  layer2_pre_3 = np.zeros_like(x) ;\n",
        "\n",
        "  # Activation functions (rest of equation 4.8)\n",
        "  h1_prime = activation_fn(layer2_pre_1)\n",
        "  h2_prime = activation_fn(layer2_pre_2)\n",
        "  h3_prime = activation_fn(layer2_pre_3)\n",
        "\n",
        "  # Weighted outputs by phi (three last terms of equation 4.9)\n",
        "  phi1_h1_prime = np.zeros_like(x) ;\n",
        "  phi2_h2_prime = np.zeros_like(x) ;\n",
        "  phi3_h3_prime = np.zeros_like(x) ;\n",
        "\n",
        "  # Combine weighted activation and add y offset (summing terms of equation 4.9)\n",
        "  y = np.zeros_like(x) ;\n",
        "\n",
        "\n",
        "  # Return everything we have calculated\n",
        "  return y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot two layer neural network as in figure 4.5 (given on page 27 in CS60010_Deep_NN.pptx (1) 2.pdf)\n",
        "def plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime):\n",
        "\n",
        "    fig, ax = plt.subplots(3,3)\n",
        "    fig.set_size_inches(8.5, 8.5)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "    ax[0,0].plot(x,layer2_pre_1,'r-'); ax[0,0].set_ylabel(r'$\\psi_{10}+\\psi_{11}h_{1}+\\psi_{12}h_{2}+\\psi_{13}h_3$')\n",
        "    ax[0,1].plot(x,layer2_pre_2,'b-'); ax[0,1].set_ylabel(r'$\\psi_{20}+\\psi_{21}h_{1}+\\psi_{22}h_{2}+\\psi_{23}h_3$')\n",
        "    ax[0,2].plot(x,layer2_pre_3,'g-'); ax[0,2].set_ylabel(r'$\\psi_{30}+\\psi_{31}h_{1}+\\psi_{32}h_{2}+\\psi_{33}h_3$')\n",
        "    ax[1,0].plot(x,h1_prime,'r-'); ax[1,0].set_ylabel(r\"$h_{1}^{'}$\")\n",
        "    ax[1,1].plot(x,h2_prime,'b-'); ax[1,1].set_ylabel(r\"$h_{2}^{'}$\")\n",
        "    ax[1,2].plot(x,h3_prime,'g-'); ax[1,2].set_ylabel(r\"$h_{3}^{'}$\")\n",
        "    ax[2,0].plot(x,phi1_h1_prime,'r-'); ax[2,0].set_ylabel(r\"$\\phi_1 h_{1}^{'}$\")\n",
        "    ax[2,1].plot(x,phi2_h2_prime,'b-'); ax[2,1].set_ylabel(r\"$\\phi_2 h_{2}^{'}$\")\n",
        "    ax[2,2].plot(x,phi3_h3_prime,'g-'); ax[2,2].set_ylabel(r\"$\\phi_3 h_{3}^{'}$\")\n",
        "\n",
        "    for plot_y in range(3):\n",
        "      for plot_x in range(3):\n",
        "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
        "        ax[plot_y,plot_x].set_aspect(0.5)\n",
        "      ax[2,plot_y].set_xlabel(r'Input, $x$');\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x,y)\n",
        "    ax.set_xlabel(r'Input, $x$'); ax.set_ylabel(r'Output, $y$')\n",
        "    ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
        "    ax.set_aspect(0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define the parameters and visualize the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameters (note first dimension of theta and phi is padded to make indices match\n",
        "# notation in book)\n",
        "theta = np.zeros([4,2])\n",
        "psi = np.zeros([4,4])\n",
        "phi = np.zeros([4,1])\n",
        "\n",
        "theta[1,0] =  0.3 ; theta[1,1] = -1.0\n",
        "theta[2,0]= -1.0  ; theta[2,1] = 2.0\n",
        "theta[3,0] = -0.5  ; theta[3,1] = 0.65\n",
        "psi[1,0] = 0.3;  psi[1,1] = 2.0; psi[1,2] = -1.0; psi[1,3]=7.0\n",
        "psi[2,0] = -0.2;  psi[2,1] = 2.0; psi[2,2] = 1.2; psi[2,3]=-8.0\n",
        "psi[3,0] = 0.3;  psi[3,1] = -2.3; psi[3,2] = -0.8; psi[3,3]=2.0\n",
        "phi[0] = 0.0; phi[1] = 0.5; phi[2] = -1.5; phi [3] = 2.2\n",
        "\n",
        "# Define a range of input values\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "# Run the neural network\n",
        "y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime \\\n",
        "    = shallow_1_1_3_3(x, ReLU, phi, psi, theta)\n",
        "\n",
        "# And then plot it\n",
        "plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do:  To test your understanding of this, consider:\n",
        "\n",
        "1.   What would happen if we increase $\\psi_{1,0}$?\n",
        "2.   What would happen if we multiplied $\\psi_{2,0}, \\psi_{2,1}, \\psi_{2,2},  \\psi_{2,3}$ by -1?\n",
        "3.  What would happen if set $\\phi_{3}$ to -1?\n",
        "\n",
        "You can rerun the code to see if you were correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **4.3 Deep neural networks**\n",
        "\n",
        "This section investigates converting neural networks to matrix form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a shallow neural network with, one input, one output, and three hidden units\n",
        "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
        "  # Initial lines\n",
        "  pre_1 = theta_10 + theta_11 * x\n",
        "  pre_2 = theta_20 + theta_21 * x\n",
        "  pre_3 = theta_30 + theta_31 * x\n",
        "  # Activation functions\n",
        "  act_1 = activation_fn(pre_1)\n",
        "  act_2 = activation_fn(pre_2)\n",
        "  act_3 = activation_fn(pre_3)\n",
        "  # Weight activations\n",
        "  w_act_1 = phi_1 * act_1\n",
        "  w_act_2 = phi_2 * act_2\n",
        "  w_act_3 = phi_3 * act_3\n",
        "  # Combine weighted activation and add y offset\n",
        "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
        "  # Return everything we have calculated\n",
        "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot the shallow neural network.  We'll assume input in is range [-1,1] and output [-1,1]\n",
        "def plot_neural(x, y):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x.T,y.T)\n",
        "  ax.set_xlabel('Input'); ax.set_ylabel('Output')\n",
        "  ax.set_xlim([-1,1]);ax.set_ylim([-1,1])\n",
        "  ax.set_aspect(1.0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a network.  We'll just consider the inputs and outputs over the range [-1,1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now lets define some parameters and run the first neural network\n",
        "n1_theta_10 = 0.0   ; n1_theta_11 = -1.0\n",
        "n1_theta_20 = 0     ; n1_theta_21 = 1.0\n",
        "n1_theta_30 = -0.67 ; n1_theta_31 =  1.0\n",
        "n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3\n",
        "\n",
        "# Define a range of input values\n",
        "n1_in = np.arange(-1,1,0.01).reshape([1,-1])\n",
        "\n",
        "# We run the neural network for each of these input values\n",
        "n1_out, *_ = shallow_1_1_3(n1_in, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
        "# And then plot it\n",
        "plot_neural(n1_in, n1_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we'll define the same neural network, but this time, we will  use matrix form as in equation 4.15.  When you get this right, it will draw the same plot as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "beta_0 = np.zeros((3,1))\n",
        "Omega_0 = np.zeros((3,1))\n",
        "beta_1 = np.zeros((1,1))\n",
        "Omega_1 = np.zeros((1,3))\n",
        "\n",
        "# TODO Fill in the values of the beta and Omega matrices with the n1_theta and n1_phi parameters that define the network above\n",
        "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0]\n",
        "# To get you started I've filled in a couple:\n",
        "beta_0[0,0] = n1_theta_10\n",
        "Omega_0[0,0] = n1_theta_11\n",
        "\n",
        "# Make sure that input data matrix has different inputs in its columns\n",
        "n_data = n1_in.size\n",
        "n_dim_in = 1\n",
        "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
        "\n",
        "# This runs the network for ALL of the inputs, x at once so we can draw graph\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,n1_in_mat))\n",
        "n1_out = beta_1 + np.matmul(Omega_1,h1)\n",
        "\n",
        "# Draw the network and check that it looks the same as the non-matrix case\n",
        "plot_neural(n1_in, n1_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we'll feed the output of the first network into the second one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now lets define some parameters and run the second neural network\n",
        "n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0\n",
        "n2_theta_20 =  0.2  ; n2_theta_21 = 1.0\n",
        "n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0\n",
        "n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0\n",
        "\n",
        "# Define a range of input values\n",
        "n2_in = np.arange(-1,1,0.01)\n",
        "\n",
        "# We run the second neural network on the output of the first network\n",
        "n2_out, *_ = \\\n",
        "    shallow_1_1_3(n1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
        "# And then plot it\n",
        "plot_neural(n1_in, n2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "beta_0 = np.zeros((3,1))\n",
        "Omega_0 = np.zeros((3,1))\n",
        "beta_1 = np.zeros((3,1))\n",
        "Omega_1 = np.zeros((3,3))\n",
        "beta_2 = np.zeros((1,1))\n",
        "Omega_2 = np.zeros((1,3))\n",
        "\n",
        "# TODO Fill in the values of the beta and Omega matrices for the n1_theta, n1_phi, n2_theta, and n2_phi parameters\n",
        "# that define the composition of the two networks above (see eqn 4.5 for Omega1 and beta1 albeit in different notation)\n",
        "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0] SO EVERYTHING IS OFFSET\n",
        "# To get you started I've filled in a few:\n",
        "beta_0[0,0] = n1_theta_10\n",
        "Omega_0[0,0] = n1_theta_11\n",
        "beta_1[0,0] = n2_theta_10 + n2_theta_11 * n1_phi_0\n",
        "Omega_1[0,0] = n2_theta_11 * n1_phi_1\n",
        "\n",
        "\n",
        "# Make sure that input data matrix has different inputs in its columns\n",
        "n_data = n1_in.size\n",
        "n_dim_in = 1\n",
        "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
        "\n",
        "# This runs the network for ALL of the inputs, x at once so we can draw graph (hence extra np.ones term)\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,n1_in_mat))\n",
        "h2 = ReLU(beta_1 + np.matmul(Omega_1,h1))\n",
        "n1_out = beta_2 + np.matmul(Omega_2,h2)\n",
        "\n",
        "# Draw the network and check that it looks the same as the non-matrix version\n",
        "plot_neural(n1_in, n1_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's make a deep network with 3 hidden layers.  It will have $D_i=4$ inputs, $D_1=5$ neurons  in the first layer, $D_2=2$ neurons in the second layer and $D_3=4$ neurons in the third layer, and $D_o = 1$ output.  Consult figure 4.6 and equations 4.15 for guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define sizes\n",
        "D_i=4; D_1=5; D_2=2; D_3=4; D_o=1\n",
        "# We'll choose the inputs and parameters of this network randomly using np.random.normal\n",
        "# For example, we'll set the input using\n",
        "n_data = 4;\n",
        "x = np.random.normal(size=(D_i, n_data))\n",
        "# TODO initialize the parameters randomly with the correct sizes\n",
        "# Replace the lines below\n",
        "beta_0 = np.random.normal(size=(1,1))\n",
        "Omega_0 = np.random.normal(size=(1,1))\n",
        "beta_1 = np.random.normal(size=(1,1))\n",
        "Omega_1 = np.random.normal(size=(1,1))\n",
        "beta_2 = np.random.normal(size=(1,1))\n",
        "Omega_2 = np.random.normal(size=(1,1))\n",
        "beta_3 = np.random.normal(size=(1,1))\n",
        "Omega_3 = np.random.normal(size=(1,1))\n",
        "\n",
        "\n",
        "# If you set the parameters to the correct sizes, the following code will run\n",
        "h1 = ReLU(beta_0 + np.matmul(Omega_0,x));\n",
        "h2 = ReLU(beta_1 + np.matmul(Omega_1,h1));\n",
        "h3 = ReLU(beta_2 + np.matmul(Omega_2,h2));\n",
        "y = beta_3 + np.matmul(Omega_3,h3)\n",
        "\n",
        "if h1.shape[0] is not D_1 or h1.shape[1] is not n_data:\n",
        "  print(\"h1 is wrong shape\")\n",
        "if h2.shape[0] is not D_2 or h1.shape[1] is not n_data:\n",
        "  print(\"h2 is wrong shape\")\n",
        "if h3.shape[0] is not D_3 or h1.shape[1] is not n_data:\n",
        "  print(\"h3 is wrong shape\")\n",
        "if y.shape[0] is not D_o or h1.shape[1] is not n_data:\n",
        "  print(\"Output is wrong shape\")\n",
        "\n",
        "# Print the inputs and outputs\n",
        "print(\"Input data points\")\n",
        "print(x)\n",
        "print (\"Output data points\")\n",
        "print(y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
