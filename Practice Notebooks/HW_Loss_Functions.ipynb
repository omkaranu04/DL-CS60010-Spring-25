{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **CS60010 - Deep Learning 2025**\n",
        "HW Assignment - Loss Functions\n",
        "\n",
        "Credits: Understanding Deep Learning by Simon J.D. Prince"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSlFkICHwHQF"
      },
      "source": [
        "# **5.1: Least Squares Loss**\n",
        "\n",
        "This section investigates the least squares loss and the equivalence of maximum likelihood and minimum negative log likelihood.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYMZ1x-Pv1ht"
      },
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# Import math Library\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv7SZR3tv7mV"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation\n",
        "\n",
        "# Define a shallow neural network\n",
        "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
        "    # Make sure that input data is (1 x n_data) array\n",
        "    n_data = x.size\n",
        "    x = np.reshape(x,(1,n_data))\n",
        "\n",
        "    # This runs the network for ALL of the inputs, x at once so we can draw graph\n",
        "    h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(omega_0,x))\n",
        "    y = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(omega_1,h1)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUT9Ain_HRim"
      },
      "outputs": [],
      "source": [
        "# Get parameters for model -- we can call this function to easily reset them\n",
        "def get_parameters():\n",
        "  # And we'll create a network that approximately fits it\n",
        "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
        "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
        "  beta_1 = np.zeros((1,1));  # formerly phi_0\n",
        "  omega_1 = np.zeros((1,3)); # formerly phi_x\n",
        "\n",
        "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
        "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
        "  beta_1[0,0] = 0.1;\n",
        "  omega_1[0,0] = -2.0; omega_1[0,1] = -1.0; omega_1[0,2] = 7.0\n",
        "\n",
        "  return beta_0, omega_0, beta_1, omega_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRR67ri_1TzN"
      },
      "outputs": [],
      "source": [
        "# Utility function for plotting data\n",
        "def plot_univariate_regression(x_model, y_model, x_data = None, y_data = None, sigma_model = None, title= None):\n",
        "  # Make sure model data are 1D arrays\n",
        "  x_model = np.squeeze(x_model)\n",
        "  y_model = np.squeeze(y_model)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x_model,y_model)\n",
        "  if sigma_model is not None:\n",
        "    ax.fill_between(x_model, y_model-2*sigma_model, y_model+2*sigma_model, color='lightgray')\n",
        "  ax.set_xlabel(r'Input, $x$'); ax.set_ylabel(r'Output, $y$')\n",
        "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
        "  ax.set_aspect(0.5)\n",
        "  if title is not None:\n",
        "    ax.set_title(title)\n",
        "  if x_data is not None:\n",
        "    ax.plot(x_data, y_data, 'ko')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsgLZwsPxauP"
      },
      "source": [
        "# Univariate regression\n",
        "\n",
        "We'll investigate a simple univariate regression situation with a single input $x$ and a single output $y$ as pictured in figures 5.4 and 5.5b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWzNOt1swFVd"
      },
      "outputs": [],
      "source": [
        "# Let's create some 1D training data\n",
        "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
        "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
        "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
        "                   0.87168699,0.58858043])\n",
        "y_train = np.array([-0.25934537,0.18195445,0.651270150,0.13921448,0.09366691,0.30567674,\\\n",
        "                    0.372291170,0.20716968,-0.08131792,0.51187806,0.16943738,0.3994327,\\\n",
        "                    0.019062570,0.55820410,0.452564960,-0.1183121,0.02957665,-1.24354444, \\\n",
        "                    0.248038840,0.26824970])\n",
        "\n",
        "# Get parameters for the model\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "sigma = 0.2\n",
        "\n",
        "# Define a range of input values\n",
        "x_model = np.arange(0,1,0.01)\n",
        "# Run the model to get values to plot and plot it.\n",
        "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVX6tl9AEXF"
      },
      "source": [
        "The blue line is the mean prediction of the model and the gray area represents plus/minus two standard deviations.  This model fits okay, but could be improved. Let's compute the loss.  We'll compute the  the least squares error, the likelihood, the negative log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaLdRlEX0FkU"
      },
      "outputs": [],
      "source": [
        "# Return probability under normal distribution\n",
        "def normal_distribution(y, mu, sigma):\n",
        "    # TODO-- write in the equation for the normal distribution\n",
        "    # Equation 5.7 from the notes (you will need np.sqrt() and np.exp(), and math.pi)\n",
        "    # Don't use the numpy version -- that's cheating!\n",
        "    # Replace the line below\n",
        "    prob = np.zeros_like(y)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TSL14dqHHbV"
      },
      "outputs": [],
      "source": [
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.119,normal_distribution(1,-1,2.3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2HcmNfUMIlj"
      },
      "outputs": [],
      "source": [
        "# Let's plot the Gaussian distribution.\n",
        "y_gauss = np.arange(-5,5,0.1)\n",
        "mu = 0; sigma = 1.0\n",
        "gauss_prob = normal_distribution(y_gauss, mu, sigma)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(y_gauss, gauss_prob)\n",
        "ax.set_xlabel(r'Input, $y$'); ax.set_ylabel(r'Probability $Pr(y)$')\n",
        "ax.set_xlim([-5,5]);ax.set_ylim([0,1.0])\n",
        "plt.show()\n",
        "\n",
        "# TODO\n",
        "# 1. Predict what will happen if we change to mu=1 and leave sigma=1\n",
        "# Now change the code above and see if you were correct.\n",
        "\n",
        "# 2. Predict what will happen if we leave mu = 0 and change sigma to 2.0\n",
        "\n",
        "# 3. Predict what will happen if we leave mu = 0 and change sigma to 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5z_0dzQMF35"
      },
      "source": [
        "Now let's compute the likelihood using this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpS7o6liCx7f"
      },
      "outputs": [],
      "source": [
        "# Return the likelihood of all of the data under the model\n",
        "def compute_likelihood(y_train, mu, sigma):\n",
        "  # TODO -- compute the likelihood of the data -- the product of the normal probabilities for each data point\n",
        "  # Top line of equation 5.3 in the notes\n",
        "  # You will need np.prod() and the normal_distribution function you used above\n",
        "  # Replace the line below\n",
        "  likelihood = 0\n",
        "\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hQxBLoVNlr2"
      },
      "outputs": [],
      "source": [
        "# Let's test this for a homoscedastic (constant sigma) model\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the mean of the Gaussian\n",
        "mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "# Set the standard deviation to something reasonable\n",
        "sigma = 0.2\n",
        "# Compute the likelihood\n",
        "likelihood = compute_likelihood(y_train, mu_pred, sigma)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000010624,likelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzphKgPfOvlk"
      },
      "source": [
        "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
        "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
        "\n",
        "This is why we use negative log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsT0CWiKBmTV"
      },
      "outputs": [],
      "source": [
        "# Return the negative log likelihood of the data under the model\n",
        "def compute_negative_log_likelihood(y_train, mu, sigma):\n",
        "  # TODO -- compute the negative log likelihood of the data without using a product\n",
        "  # In other words, compute minus one times the sum of the log probabilities\n",
        "  # Equation 5.4 in the notes\n",
        "  # You will need np.sum(), np.log()\n",
        "  # Replace the line below\n",
        "  nll = 0\n",
        "\n",
        "  return nll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVxUXg9rQmwI"
      },
      "outputs": [],
      "source": [
        "# Let's test this for a homoscedastic (constant sigma) model\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the mean of the Gaussian\n",
        "mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "# Set the standard deviation to something reasonable\n",
        "sigma = 0.2\n",
        "# Compute the negative log likelihood\n",
        "nll = compute_negative_log_likelihood(y_train, mu_pred, sigma)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(11.452419564,nll))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S8bXApoWVLG"
      },
      "source": [
        "For good measure, let's compute the sum of squares as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1pjFdHCF4JZ"
      },
      "outputs": [],
      "source": [
        "# Return the squared distance between the observed data (y_train) and the prediction of the model (y_pred)\n",
        "def compute_sum_of_squares(y_train, y_pred):\n",
        "  # TODO -- compute the sum of squared distances between the training data and the model prediction\n",
        "  # Eqn 5.10 in the notes.  Make sure that you understand this, and ask questions if you don't\n",
        "  # Replace the line below\n",
        "  sum_of_squares = 0;\n",
        "\n",
        "  return sum_of_squares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C40fskIHBx7"
      },
      "outputs": [],
      "source": [
        "# Let's test this again\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the mean of the Gaussian, which is out best prediction of y\n",
        "y_pred = mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "# Compute the sum of squares\n",
        "sum_of_squares = compute_sum_of_squares(y_train, y_pred)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(2.020992572,sum_of_squares))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgcRojvPWh4V"
      },
      "source": [
        "Now let's investigate finding the maximum likelihood / minimum negative log likelihood / least squares solution.  For simplicity, we'll assume that all the parameters are correct except one and look at how the likelihood, negative log likelihood, and sum of squares change as we manipulate the last parameter.  We'll start with overall y offset, beta_1 (formerly phi_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFKtDaAeVU4U"
      },
      "outputs": [],
      "source": [
        "# Define a range of values for the parameter\n",
        "beta_1_vals = np.arange(0,1.0,0.01)\n",
        "# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares\n",
        "likelihoods = np.zeros_like(beta_1_vals)\n",
        "nlls = np.zeros_like(beta_1_vals)\n",
        "sum_squares = np.zeros_like(beta_1_vals)\n",
        "\n",
        "# Initialise the parameters\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "sigma = 0.2\n",
        "for count in range(len(beta_1_vals)):\n",
        "  # Set the value for the parameter\n",
        "  beta_1[0,0] = beta_1_vals[count]\n",
        "  # Run the network with new parameters\n",
        "  mu_pred = y_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "  # Compute and store the three values\n",
        "  likelihoods[count] = compute_likelihood(y_train, mu_pred, sigma)\n",
        "  nlls[count] = compute_negative_log_likelihood(y_train, mu_pred, sigma)\n",
        "  sum_squares[count] = compute_sum_of_squares(y_train, y_pred)\n",
        "  # Draw the model for every 20th parameter setting\n",
        "  if count % 20 == 0:\n",
        "    # Run the model to get values to plot and plot it.\n",
        "    y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "    plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta1=%3.3f\"%(beta_1[0,0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHXeTa9MagO6"
      },
      "outputs": [],
      "source": [
        "# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the offset beta1\n",
        "fig, ax = plt.subplots(1,2)\n",
        "fig.set_size_inches(10.5, 5.5)\n",
        "fig.tight_layout(pad=10.0)\n",
        "likelihood_color = 'tab:red'\n",
        "nll_color = 'tab:blue'\n",
        "\n",
        "ax[0].set_xlabel('beta_1[0]')\n",
        "ax[0].set_ylabel('likelihood', color = likelihood_color)\n",
        "ax[0].plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
        "ax[0].tick_params(axis='y', labelcolor=likelihood_color)\n",
        "\n",
        "ax00 = ax[0].twinx()\n",
        "ax00.plot(beta_1_vals, nlls, color = nll_color)\n",
        "ax00.set_ylabel('negative log likelihood', color = nll_color)\n",
        "ax00.tick_params(axis='y', labelcolor = nll_color)\n",
        "\n",
        "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
        "\n",
        "ax[1].plot(beta_1_vals, sum_squares); ax[1].set_xlabel('beta_1[0]'); ax[1].set_ylabel('sum of squares')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDEPhddNdN4u"
      },
      "outputs": [],
      "source": [
        "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
        "# and the least squares solutions\n",
        "# Let's check that:\n",
        "print(\"Maximum likelihood = %3.3f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
        "print(\"Minimum negative log likelihood = %3.3f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
        "print(\"Least squares = %3.3f, at beta_1=%3.3f\"%( (sum_squares[np.argmin(sum_squares)],beta_1_vals[np.argmin(sum_squares)])))\n",
        "\n",
        "# Plot the best model\n",
        "beta_1[0,0] = beta_1_vals[np.argmin(sum_squares)]\n",
        "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta1=%3.3f\"%(beta_1[0,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "771G8N1Vk5A2"
      },
      "source": [
        "They all give the same answer. But you can see from the three plots above that the likelihood is very small unless the parameters are almost correct.  So in practice, we would work with the negative log likelihood or the least squares.<br>\n",
        "\n",
        "Let's do the same thing with the standard deviation parameter of our network.  This is not an output of the network (unless we choose to make that the case), but it still affects the likelihood.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMNAr0R8gg82"
      },
      "outputs": [],
      "source": [
        "# Define a range of values for the parameter\n",
        "sigma_vals = np.arange(0.1,0.5,0.005)\n",
        "# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares\n",
        "likelihoods = np.zeros_like(sigma_vals)\n",
        "nlls = np.zeros_like(sigma_vals)\n",
        "sum_squares = np.zeros_like(sigma_vals)\n",
        "\n",
        "# Initialise the parameters\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Might as well set to the best offset\n",
        "beta_1[0,0] = 0.27\n",
        "for count in range(len(sigma_vals)):\n",
        "  # Set the value for the parameter\n",
        "  sigma = sigma_vals[count]\n",
        "  # Run the network with new parameters\n",
        "  mu_pred = y_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "  # Compute and store the three values\n",
        "  likelihoods[count] = compute_likelihood(y_train, mu_pred, sigma)\n",
        "  nlls[count] = compute_negative_log_likelihood(y_train, mu_pred, sigma)\n",
        "  sum_squares[count] = compute_sum_of_squares(y_train, y_pred)\n",
        "  # Draw the model for every 20th parameter setting\n",
        "  if count % 20 == 0:\n",
        "    # Run the model to get values to plot and plot it.\n",
        "    y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "    plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model=sigma, title=\"sigma=%3.3f\"%(sigma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9jduyHLDAZC"
      },
      "outputs": [],
      "source": [
        "# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the standard deviation sigma\n",
        "fig, ax = plt.subplots(1,2)\n",
        "fig.set_size_inches(10.5, 5.5)\n",
        "fig.tight_layout(pad=10.0)\n",
        "likelihood_color = 'tab:red'\n",
        "nll_color = 'tab:blue'\n",
        "\n",
        "\n",
        "ax[0].set_xlabel('sigma')\n",
        "ax[0].set_ylabel('likelihood', color = likelihood_color)\n",
        "ax[0].plot(sigma_vals, likelihoods, color = likelihood_color)\n",
        "ax[0].tick_params(axis='y', labelcolor=likelihood_color)\n",
        "\n",
        "ax00 = ax[0].twinx()\n",
        "ax00.plot(sigma_vals, nlls, color = nll_color)\n",
        "ax00.set_ylabel('negative log likelihood', color = nll_color)\n",
        "ax00.tick_params(axis='y', labelcolor = nll_color)\n",
        "\n",
        "plt.axvline(x = sigma_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
        "\n",
        "ax[1].plot(sigma_vals, sum_squares); ax[1].set_xlabel('sigma'); ax[1].set_ylabel('sum of squares')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH7yER52Dxt5"
      },
      "outputs": [],
      "source": [
        "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
        "# The least squares solution does not depend on sigma, so it's just flat -- no use here.\n",
        "# Let's check that:\n",
        "print(\"Maximum likelihood = %3.3f, at sigma=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],sigma_vals[np.argmax(likelihoods)])))\n",
        "print(\"Minimum negative log likelihood = %3.3f, at sigma=%3.3f\"%( (nlls[np.argmin(nlls)],sigma_vals[np.argmin(nlls)])))\n",
        "# Plot the best model\n",
        "sigma= sigma_vals[np.argmin(nlls)]\n",
        "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta_1=%3.3f, sigma =%3.3f\"%(beta_1[0,0],sigma))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_KeGNAHEbIt"
      },
      "source": [
        "Obviously, to fit the full neural model we would vary all of the 10 parameters of the network in $\\boldsymbol\\beta_{0},\\boldsymbol\\Omega_{0},\\boldsymbol\\beta_{1},\\boldsymbol\\Omega_{1}$ (and maybe $\\sigma$) until we find the combination that have the maximum likelihood / minimum negative log likelihood / least squares.<br><br>\n",
        "\n",
        "Here we just varied one at a time as it is easier to see what is going on.  This is known as **coordinate descent**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syHwLReRjmCl"
      },
      "source": [
        "# **5.2 Binary Cross-Entropy Loss**\n",
        "\n",
        "Here we investigate the binary cross-entropy loss.  It follows from applying the formula in section 5.2 to a loss function based on the Bernoulli distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzTMylTQjmCl"
      },
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# Import math Library\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeoprDoWjmCm"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation\n",
        "\n",
        "# Define a shallow neural network\n",
        "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
        "    # Make sure that input data is (1 x n_data) array\n",
        "    n_data = x.size\n",
        "    x = np.reshape(x,(1,n_data))\n",
        "\n",
        "    # This runs the network for ALL of the inputs, x at once so we can draw graph\n",
        "    h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(omega_0,x))\n",
        "    model_out = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(omega_1,h1)\n",
        "    return model_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QHJsP4GjmCm"
      },
      "outputs": [],
      "source": [
        "# Get parameters for model -- we can call this function to easily reset them\n",
        "def get_parameters():\n",
        "  # And we'll create a network that approximately fits it\n",
        "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
        "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
        "  beta_1 = np.zeros((1,1));  # formerly phi_0\n",
        "  omega_1 = np.zeros((1,3)); # formerly phi_x\n",
        "\n",
        "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
        "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
        "  beta_1[0,0] = 2.6;\n",
        "  omega_1[0,0] = -24.0; omega_1[0,1] = -8.0; omega_1[0,2] = 50.0\n",
        "\n",
        "  return beta_0, omega_0, beta_1, omega_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5z_-Z76jmCm"
      },
      "outputs": [],
      "source": [
        "# Utility function for plotting data\n",
        "def plot_binary_classification(x_model, out_model, lambda_model, x_data = None, y_data = None, title= None):\n",
        "  # Make sure model data are 1D arrays\n",
        "  x_model = np.squeeze(x_model)\n",
        "  out_model = np.squeeze(out_model)\n",
        "  lambda_model = np.squeeze(lambda_model)\n",
        "\n",
        "  fig, ax = plt.subplots(1,2)\n",
        "  fig.set_size_inches(7.0, 3.5)\n",
        "  fig.tight_layout(pad=3.0)\n",
        "  ax[0].plot(x_model,out_model)\n",
        "  ax[0].set_xlabel(r'Input, $x$'); ax[0].set_ylabel(r'Model output')\n",
        "  ax[0].set_xlim([0,1]);ax[0].set_ylim([-4,4])\n",
        "  if title is not None:\n",
        "    ax[0].set_title(title)\n",
        "  ax[1].plot(x_model,lambda_model)\n",
        "  ax[1].set_xlabel(r'Input, $x$'); ax[1].set_ylabel(r'$\\lambda$ or Pr(y=1|x)')\n",
        "  ax[1].set_xlim([0,1]);ax[1].set_ylim([-0.05,1.05])\n",
        "  if title is not None:\n",
        "    ax[1].set_title(title)\n",
        "  if x_data is not None:\n",
        "    ax[1].plot(x_data, y_data, 'ko')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjoR7h0QjmCm"
      },
      "source": [
        "# Binary classification\n",
        "\n",
        "In binary classification tasks, the network predicts the probability of the output belonging to class 1.  Since probabilities must lie in [0,1] and the network can output arbitrary values, we map the network through a sigmoid function that ensures the range is valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFb8h-9IXnIe"
      },
      "outputs": [],
      "source": [
        "# Sigmoid function that maps [-infty,infty] to [0,1]\n",
        "def sigmoid(model_out):\n",
        "  # TODO -- implement the logistic sigmoid function from equation 5.18\n",
        "  # Replace this line:\n",
        "  sig_model_out = np.zeros_like(model_out)\n",
        "\n",
        "  return sig_model_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP0eaVDdjmCm"
      },
      "outputs": [],
      "source": [
        "# Let's create some 1D training data\n",
        "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
        "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
        "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
        "                   0.87168699,0.58858043])\n",
        "y_train = np.array([0,1,1,0,0,1,\\\n",
        "                    1,0,0,1,0,1,\\\n",
        "                    0,1,1,0,1,0, \\\n",
        "                    1,1])\n",
        "\n",
        "# Get parameters for the model\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "\n",
        "# Define a range of input values\n",
        "x_model = np.arange(0,1,0.01)\n",
        "# Run the model to get values to plot and plot it.\n",
        "model_out= shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_model = sigmoid(model_out)\n",
        "plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkxRN6bFjmCn"
      },
      "source": [
        "The left is model output and the right is the model output after the sigmoid has been applied, so it now lies in the range [0,1] and represents the probability, that y=1.  The black dots show the training data.  We'll compute the likelihood and the negative log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWSabL-hjmCn"
      },
      "outputs": [],
      "source": [
        "# Return probability under Bernoulli distribution for observed class y\n",
        "def bernoulli_distribution(y, lambda_param):\n",
        "    # TODO-- write in the equation for the Bernoulli distribution\n",
        "    # Equation 5.17 from the notes (you will need np.power)\n",
        "    # Replace the line below\n",
        "    prob = np.zeros_like(y)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhXN-IGLjmCn"
      },
      "outputs": [],
      "source": [
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.8,bernoulli_distribution(0,0.2)))\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.2,bernoulli_distribution(1,0.2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD3IqsIBjmCn"
      },
      "source": [
        "Now let's compute the likelihood using this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4098UCnpjmCn"
      },
      "outputs": [],
      "source": [
        "# Return the likelihood of all of the data under the model\n",
        "def compute_likelihood(y_train, lambda_param):\n",
        "  # TODO -- compute the likelihood of the data -- the product of the Bernoulli probabilities for each data point\n",
        "  # Top line of equation 5.3 in the notes\n",
        "  # You will need np.prod() and the bernoulli_distribution function you used above\n",
        "  # Replace the line below\n",
        "  likelihood = 0\n",
        "\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANj0uDP1jmCn"
      },
      "outputs": [],
      "source": [
        "# Let's test this\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the Bernoulli parameter lambda\n",
        "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_train = sigmoid(model_out)\n",
        "# Compute the likelihood\n",
        "likelihood = compute_likelihood(y_train, lambda_train)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000070237,likelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdzX5qe4jmCo"
      },
      "source": [
        "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
        "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
        "\n",
        "This is why we use negative log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yIj--fAjmCo"
      },
      "outputs": [],
      "source": [
        "# Return the negative log likelihood of the data under the model\n",
        "def compute_negative_log_likelihood(y_train, lambda_param):\n",
        "  # TODO -- compute the likelihood of the data -- don't use the likelihood function above -- compute the negative sum of the log probabilities\n",
        "  # You will need np.sum(), np.log()\n",
        "  # Replace the line below\n",
        "  nll = 0\n",
        "\n",
        "  return nll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEonkw_8jmCo"
      },
      "outputs": [],
      "source": [
        "# Let's test this\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the mean of the Gaussian\n",
        "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "# Pass through the sigmoid function\n",
        "lambda_train = sigmoid(model_out)\n",
        "# Compute the log likelihood\n",
        "nll = compute_negative_log_likelihood(y_train, lambda_train)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(9.563639387,nll))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kWlLuSyjmCo"
      },
      "source": [
        "Now let's investigate finding the maximum likelihood / minimum negative log likelihood solution.  For simplicity, we'll assume that all the parameters are fixed except one and look at how the likelihood and negative log likelihood change as we manipulate the last parameter.  We'll start with overall y_offset, beta_1 (formerly phi_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdJPWuifjmCo"
      },
      "outputs": [],
      "source": [
        "# Define a range of values for the parameter\n",
        "beta_1_vals = np.arange(-2,6.0,0.1)\n",
        "# Create some arrays to store the likelihoods, negative log likelihoods\n",
        "likelihoods = np.zeros_like(beta_1_vals)\n",
        "nlls = np.zeros_like(beta_1_vals)\n",
        "\n",
        "# Initialise the parameters\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "for count in range(len(beta_1_vals)):\n",
        "  # Set the value for the parameter\n",
        "  beta_1[0,0] = beta_1_vals[count]\n",
        "  # Run the network with new parameters\n",
        "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "  lambda_train = sigmoid(model_out)\n",
        "  # Compute and store the two values\n",
        "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
        "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
        "  # Draw the model for every 20th parameter setting\n",
        "  if count % 20 == 0:\n",
        "    # Run the model to get values to plot and plot it.\n",
        "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "    lambda_model = sigmoid(model_out)\n",
        "    plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta_1[0]=%3.3f\"%(beta_1[0,0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytAK6vV2jmCo"
      },
      "outputs": [],
      "source": [
        "# Now let's plot the likelihood and negative log likelihood as a function of the value of the offset beta1\n",
        "fig, ax = plt.subplots()\n",
        "fig.tight_layout(pad=5.0)\n",
        "likelihood_color = 'tab:red'\n",
        "nll_color = 'tab:blue'\n",
        "\n",
        "\n",
        "ax.set_xlabel('beta_1[0]')\n",
        "ax.set_ylabel('likelihood', color = likelihood_color)\n",
        "ax.plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
        "ax.tick_params(axis='y', labelcolor=likelihood_color)\n",
        "\n",
        "ax1 = ax.twinx()\n",
        "ax1.plot(beta_1_vals, nlls, color = nll_color)\n",
        "ax1.set_ylabel('negative log likelihood', color = nll_color)\n",
        "ax1.tick_params(axis='y', labelcolor = nll_color)\n",
        "\n",
        "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06yPhvVEjmCp"
      },
      "outputs": [],
      "source": [
        "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
        "# Let's check that:\n",
        "print(\"Maximum likelihood = %f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
        "print(\"Minimum negative log likelihood = %f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
        "\n",
        "# Plot the best model\n",
        "beta_1[0,0] = beta_1_vals[np.argmin(nlls)]\n",
        "model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_model = sigmoid(model_out)\n",
        "plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta_1[0]=%3.3f\"%(beta_1[0,0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDF1ULFIjmCp"
      },
      "source": [
        "They both give the same answer. But you can see from the likelihood above that the likelihood is very small unless the parameters are almost correct.  So in practice, we would work with the negative log likelihood.<br><br>\n",
        "\n",
        "Again, to fit the full neural model we would vary all of the 10 parameters of the network in the $\\boldsymbol\\beta_{0},\\boldsymbol\\Omega_{0},\\boldsymbol\\beta_{1},\\boldsymbol\\Omega_{1}$ until we find the combination that have the maximum likelihood / minimum negative log likelihood.<br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vc7bIgsj0xz"
      },
      "source": [
        "# **5.3 Multiclass Cross-Entropy Loss**\n",
        "\n",
        "Here we investigate the multi-class cross-entropy loss.  It follows from applying the formula in section 5.2 to a loss function based on the Categorical distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm2AXn6Fj0xz"
      },
      "outputs": [],
      "source": [
        "# Imports math library\n",
        "import numpy as np\n",
        "# Used for repmat\n",
        "import numpy.matlib\n",
        "# Imports plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# Import math Library\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt9RRQ05j0xz"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation\n",
        "\n",
        "# Define a shallow neural network\n",
        "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
        "    # Make sure that input data is (1 x n_data) array\n",
        "    n_data = x.size\n",
        "    x = np.reshape(x,(1,n_data))\n",
        "\n",
        "    # This runs the network for ALL of the inputs, x at once so we can draw graph\n",
        "    h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(omega_0,x))\n",
        "    model_out = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(omega_1,h1)\n",
        "    return model_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrgMHYtUj0xz"
      },
      "outputs": [],
      "source": [
        "# Get parameters for model -- we can call this function to easily reset them\n",
        "def get_parameters():\n",
        "  # And we'll create a network that approximately fits it\n",
        "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
        "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
        "  beta_1 = np.zeros((3,1));  # NOTE -- there are three outputs now (one for each class, so three output biases)\n",
        "  omega_1 = np.zeros((3,3)); # NOTE -- there are three outputs now (one for each class, so nine output weights, connecting 3 hidden units to 3 outputs)\n",
        "\n",
        "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
        "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
        "  beta_1[0,0] = 2.0; beta_1[1,0] = -2; beta_1[2,0] = 0.0\n",
        "  omega_1[0,0] = -24.0; omega_1[0,1] = -8.0; omega_1[0,2] = 50.0\n",
        "  omega_1[1,0] = -2.0; omega_1[1,1] = 8.0; omega_1[1,2] = -30.0\n",
        "  omega_1[2,0] = 16.0; omega_1[2,1] = -8.0; omega_1[2,2] =-8\n",
        "\n",
        "  return beta_0, omega_0, beta_1, omega_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqopILJ2j0x0"
      },
      "outputs": [],
      "source": [
        "# Utility function for plotting data\n",
        "def plot_multiclass_classification(x_model, out_model, lambda_model, x_data = None, y_data = None, title= None):\n",
        "  # Make sure model data are 1D arrays\n",
        "  n_data = len(x_model)\n",
        "  n_class = 3\n",
        "  x_model = np.squeeze(x_model)\n",
        "  out_model = np.reshape(out_model, (n_class,n_data))\n",
        "  lambda_model = np.reshape(lambda_model, (n_class,n_data))\n",
        "\n",
        "  fig, ax = plt.subplots(1,2)\n",
        "  fig.set_size_inches(7.0, 3.5)\n",
        "  fig.tight_layout(pad=3.0)\n",
        "  ax[0].plot(x_model,out_model[0,:],'r-')\n",
        "  ax[0].plot(x_model,out_model[1,:],'g-')\n",
        "  ax[0].plot(x_model,out_model[2,:],'b-')\n",
        "  ax[0].set_xlabel('Input, $x$'); ax[0].set_ylabel('Model outputs')\n",
        "  ax[0].set_xlim([0,1]);ax[0].set_ylim([-4,4])\n",
        "  if title is not None:\n",
        "    ax[0].set_title(title)\n",
        "  ax[1].plot(x_model,lambda_model[0,:],'r-')\n",
        "  ax[1].plot(x_model,lambda_model[1,:],'g-')\n",
        "  ax[1].plot(x_model,lambda_model[2,:],'b-')\n",
        "  ax[1].set_xlabel('Input, $x$'); ax[1].set_ylabel('$\\lambda$ or Pr(y=k|x)')\n",
        "  ax[1].set_xlim([0,1]);ax[1].set_ylim([-0.1,1.05])\n",
        "  if title is not None:\n",
        "    ax[1].set_title(title)\n",
        "  if x_data is not None:\n",
        "    for i in range(len(x_data)):\n",
        "      if y_data[i] ==0:\n",
        "        ax[1].plot(x_data[i],-0.05, 'r.')\n",
        "      if y_data[i] ==1:\n",
        "        ax[1].plot(x_data[i],-0.05, 'g.')\n",
        "      if y_data[i] ==2:\n",
        "        ax[1].plot(x_data[i],-0.05, 'b.')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn8x2hRGj0x0"
      },
      "source": [
        "# Multiclass classification\n",
        "\n",
        "For multiclass classification, the network must predict the probability of $K$ classes, using $K$ outputs.  However, these probability must be non-negative and sum to one, and the network outputs can take arbitrary values.  Hence, we pass the outputs through a softmax function which maps $K$ arbitrary values to $K$ non-negative values that sum to one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VigFbh7qj0x0"
      },
      "outputs": [],
      "source": [
        "# Softmax function that maps a vector of arbitrary values to a vector of values that are positive and sum to one.\n",
        "def softmax(model_out):\n",
        "  # This operation has to be done separately for every column of the input\n",
        "  # Compute exponentials of all the elements\n",
        "  # TODO: compute the softmax function (eq 5.22)\n",
        "  # Replace this skeleton code\n",
        "\n",
        "  # Compute the exponential of the model outputs\n",
        "  exp_model_out = np.zeros_like(model_out) ;\n",
        "  # Compute the sum of the exponentials (denominator of equation 5.22)\n",
        "  sum_exp_model_out = np.zeros_like(model_out) ;\n",
        "  # Normalize the exponentials (np.matlib.repmat might be useful here)\n",
        "  softmax_model_out = np.ones_like(model_out)/ exp_model_out.shape[0]\n",
        "\n",
        "  return softmax_model_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKKVIUXwj0x0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Let's create some 1D training data\n",
        "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
        "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
        "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
        "                   0.87168699,0.58858043])\n",
        "y_train = np.array([2,0,1,2,1,0,\\\n",
        "                    0,2,2,0,2,0,\\\n",
        "                    2,0,1,2,1,2, \\\n",
        "                    1,0])\n",
        "\n",
        "# Get parameters for the model\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "\n",
        "# Define a range of input values\n",
        "x_model = np.arange(0,1,0.01)\n",
        "# Run the model to get values to plot and plot it.\n",
        "model_out= shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_model = softmax(model_out)\n",
        "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8eyFzuCj0x0"
      },
      "source": [
        "The left is model output and the right is the model output after the softmax has been applied, so it now lies in the range [0,1] and represents the probability, that y=0 (red), 1 (green) and 2 (blue).  The dots at the bottom show the training data with the same color scheme.  So we want the red curve to be high where there are red dots, the green curve to be high where there are green dots, and the blue curve to be high where there are blue dots  We'll compute the likelihood and the negative log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt-5rPxoj0x0"
      },
      "outputs": [],
      "source": [
        "# Return probability under categorical distribution for observed class y\n",
        "# Just take value from row k of lambda param where y =k,\n",
        "def categorical_distribution(y, lambda_param):\n",
        "    return np.array([lambda_param[row, i] for i, row in enumerate (y)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAe24qCEj0x0"
      },
      "outputs": [],
      "source": [
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.2,categorical_distribution(np.array([[0]]),np.array([[0.2],[0.5],[0.3]]))))\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.5,categorical_distribution(np.array([[1]]),np.array([[0.2],[0.5],[0.3]]))))\n",
        "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.3,categorical_distribution(np.array([[2]]),np.array([[0.2],[0.5],[0.3]]))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw-ntkeZj0x0"
      },
      "source": [
        "Now let's compute the likelihood using this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9CQ31Ylj0x1"
      },
      "outputs": [],
      "source": [
        "# Return the likelihood of all of the data under the model\n",
        "def compute_likelihood(y_train, lambda_param):\n",
        "  # TODO -- compute the likelihood of the data -- the product of the categorical probabilities for each data point\n",
        "  # Top line of equation 5.3 in the notes\n",
        "  # You will need np.prod() and the categorical_distribution function you used above\n",
        "  # Replace the line below\n",
        "  likelihood = 0\n",
        "\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlOw7dmNj0x1"
      },
      "outputs": [],
      "source": [
        "# Let's test this\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the parameters of the categorical distribution\n",
        "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_train = softmax(model_out)\n",
        "# Compute the likelihood\n",
        "likelihood = compute_likelihood(y_train, lambda_train)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000000041,likelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJpoQM9Hj0x1"
      },
      "source": [
        "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
        "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
        "\n",
        "This is why we use negative log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zerjxM_Vj0x1"
      },
      "outputs": [],
      "source": [
        "# Return the negative log likelihood of the data under the model\n",
        "def compute_negative_log_likelihood(y_train, lambda_param):\n",
        "  # TODO -- compute the negative log likelihood of the data -- don't use the likelihood function above -- compute the negative sum of the log probabilities\n",
        "  # You will need np.sum(), np.log()\n",
        "  # Replace the line below\n",
        "  nll = 0\n",
        "\n",
        "  return nll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXi2x7sxj0x1"
      },
      "outputs": [],
      "source": [
        "# Let's test this\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "# Use our neural network to predict the parameters of the categorical distribution\n",
        "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "# Pass the outputs through the softmax function\n",
        "lambda_train = softmax(model_out)\n",
        "# Compute the negative log likelihood\n",
        "nll = compute_negative_log_likelihood(y_train, lambda_train)\n",
        "# Let's double check we get the right answer before proceeding\n",
        "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(17.015457867,nll))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnj2K7rvj0x2"
      },
      "source": [
        "Now let's investigate finding the maximum likelihood / minimum negative log likelihood solution.  For simplicity, we'll assume that all the parameters are fixed except one and look at how the likelihood and negative log likelihood change as we manipulate the last parameter.  We'll start with overall y_offset, $\\beta_1$ (formerly $\\phi_0$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuL0Iu1Xj0x2"
      },
      "outputs": [],
      "source": [
        "# Define a range of values for the parameter\n",
        "beta_1_vals = np.arange(-2,6.0,0.1)\n",
        "# Create some arrays to store the likelihoods, negative log likelihoods\n",
        "likelihoods = np.zeros_like(beta_1_vals)\n",
        "nlls = np.zeros_like(beta_1_vals)\n",
        "\n",
        "# Initialise the parameters\n",
        "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
        "for count in range(len(beta_1_vals)):\n",
        "  # Set the value for the parameter\n",
        "  beta_1[0,0] = beta_1_vals[count]\n",
        "  # Run the network with new parameters\n",
        "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
        "  lambda_train = softmax(model_out)\n",
        "  # Compute and store the two values\n",
        "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
        "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
        "  # Draw the model for every 20th parameter setting\n",
        "  if count % 20 == 0:\n",
        "    # Run the model to get values to plot and plot it.\n",
        "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "    lambda_model = softmax(model_out)\n",
        "    plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUzc83TGj0x2"
      },
      "outputs": [],
      "source": [
        "# Now let's plot the likelihood and negative log likelihood as a function of the value of the offset beta1\n",
        "fig, ax = plt.subplots()\n",
        "fig.tight_layout(pad=5.0)\n",
        "likelihood_color = 'tab:red'\n",
        "nll_color = 'tab:blue'\n",
        "\n",
        "\n",
        "ax.set_xlabel('beta_1[0, 0]')\n",
        "ax.set_ylabel('likelihood', color = likelihood_color)\n",
        "ax.plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
        "ax.tick_params(axis='y', labelcolor=likelihood_color)\n",
        "\n",
        "ax1 = ax.twinx()\n",
        "ax1.plot(beta_1_vals, nlls, color = nll_color)\n",
        "ax1.set_ylabel('negative log likelihood', color = nll_color)\n",
        "ax1.tick_params(axis='y', labelcolor = nll_color)\n",
        "\n",
        "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sPJjwMWj0x2"
      },
      "outputs": [],
      "source": [
        "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood solution\n",
        "# Let's check that:\n",
        "print(\"Maximum likelihood = %f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
        "print(\"Minimum negative log likelihood = %f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
        "\n",
        "# Plot the best model\n",
        "beta_1[0,0] = beta_1_vals[np.argmin(nlls)]\n",
        "model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
        "lambda_model = softmax(model_out)\n",
        "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSECUFH4j0x2"
      },
      "source": [
        "They both give the same answer. But you can see from the likelihood above that the likelihood is very small unless the parameters are almost correct.  So in practice, we would work with the negative log likelihood.<br><br>\n",
        "\n",
        "Again, to fit the full neural model we would vary all of the 16 parameters of the network in the $\\boldsymbol\\beta_{0},\\boldsymbol\\Omega_{0},\\boldsymbol\\beta_{1},\\boldsymbol\\Omega_{1}$ until we find the combination that have the maximum likelihood / minimum negative log likelihood.<br><br>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
